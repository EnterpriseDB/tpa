---

# © Copyright EnterpriseDB UK Limited 2015-2025 - All rights reserved.

# NOTE: This test is special, because upgrade uses it directly
# as a pre-update health check of the cluster. It should test only the
# fundamental properties we expect from the haproxy configuration.

- assert:
    msg: "This test may be applied only to haproxy instances"
    that:
    - role|contains('haproxy')

# First, we check that connections through haproxy to the backend
# database are TLS-protected from end-to-end (as a side effect, we
# ensure that we can connect and issue queries too).

- name: Ensure that connections through haproxy are TLS-protected
  postgresql_query:
    conninfo: "{{ dsn|dbname('postgres', user=postgres_user, host='127.0.0.1') }}"
    query: >
      SELECT ssl
      FROM pg_stat_activity sa JOIN pg_stat_ssl ss USING (pid)
      WHERE pid = pg_backend_pid()
  register: q
  failed_when:
    q is not successful or not q.ssl
  vars:
    _task_environment:
      PGPASSWORD: "{{ vars['%s_password' % postgres_user] }}"
  environment: "{{ target_environment|combine(_task_environment) }}"

# We issue "show stat" to the haproxy stats socket, and expect to get
# back results like the following:
#
#     # pxname,svname,…
#     fe,FRONTEND,…,STATUS,…
#     be,hostname,…,STATUS,…
#     be,hostname,…,STATUS,…
#     be,BACKEND,…,STATUS,…
#
# The first (commented) line is a list of column names, most of which we
# don't care about (see contrib/prometheus-exporter/service-prometheus.c
# in the haproxy source for an explanation of the column names). But we
# build a table of results in order to verify that the frontends and
# backends are all working as expected.
#
# The "pxname" (proxy name) should be either "fe" or "be", corresponding
# to the definitions in haproxy.cfg. The fe proxy should have one entry
# with svname FRONTEND; the be proxy should have an overall entry with
# svname BACKEND, plus one entry per server in haproxy_backend_servers.

- name: Fetch haproxy statistics
  shell: >
    echo 'show stat'|socat {{ haproxy_stats_socket }} stdio
  register: hstat
  become_user: root
  become: yes

- name: Build haproxy_stats results array
  set_fact:
    haproxy_stats: "{{
        haproxy_stats|default([])|union([
          item|from_csv(column_names)
        ])
      }}"
  with_items: "{{ hstat.stdout_lines }}"
  when:
    not item.startswith('#')
  vars:
    column_names: "{{ hstat.stdout_lines[0][2:].split(',') }}"

- name: Ensure haproxy frontend is up
  assert:
    msg: "Expected one working haproxy frontend"
    that:
    - frontends|length == 1
    - frontends|map(attribute='status')|first == 'OPEN'
  vars:
    frontends: >-
      {{ haproxy_stats|selectattr('pxname', 'equalto', 'fe')|selectattr('svname', 'equalto', 'FRONTEND')|list }}

- name: Ensure haproxy backend is up
  assert:
    msg: "Expected one working haproxy backend with {{ num_servers_wanted }} servers"
    that:
    - backends|length == 1
    - backends|map(attribute='status')|first == 'UP'
    - servers|length == num_servers_wanted|int
  vars:
    backends: >-
      {{ haproxy_stats|selectattr('pxname', 'equalto', 'be')|selectattr('svname', 'equalto', 'BACKEND')|list }}
    servers: >-
      {{ haproxy_stats|selectattr('pxname', 'equalto', 'be')|rejectattr('svname', 'equalto', 'BACKEND')|list }}
    num_servers_wanted: "{{ haproxy_backend_servers|length }}"

- name: Ensure haproxy read-only frontend is up
  assert:
    msg: "Expected one working haproxy frontend"
    that:
    - frontends|length == 1
    - frontends|map(attribute='status')|first == 'OPEN'
  vars:
    frontends: >-
      {{ haproxy_stats|selectattr('pxname', 'equalto', 'fe_ro')|selectattr('svname', 'equalto', 'FRONTEND')|list }}
  when:
    haproxy_read_only_load_balancer_enabled|default(false)

- name: Ensure haproxy read-only backend is up
  assert:
    msg: "Expected one working haproxy backend with {{ num_servers_wanted }} servers"
    that:
    - backends|length == 1
    - backends|map(attribute='status')|first == 'UP'
    - servers|length == num_servers_wanted|int
  vars:
    backends: >-
      {{ haproxy_stats|selectattr('pxname', 'equalto', 'be_ro')|selectattr('svname', 'equalto', 'BACKEND')|list }}
    servers: >-
      {{ haproxy_stats|selectattr('pxname', 'equalto', 'be_ro')|rejectattr('svname', 'equalto', 'BACKEND')|list }}
    num_servers_wanted: "{{ haproxy_backend_servers|length }}"
  when:
    haproxy_read_only_load_balancer_enabled|default(false)

- name: Ensure each server in haproxy_backend_servers is up
  assert:
    msg: >-
      Expected haproxy_backend_servers={{ ','.join(haproxy_backend_servers) }} to be UP,
      got {% for s in servers %}{{ s.svname }}={{ s.status }}{{ loop.last|ternary('', ', ') }}{% endfor %}
    that:
    - servers|map(attribute='svname')|sort == haproxy_backend_servers|sort
    - up_servers == num_servers_wanted
  vars:
    num_servers: >
      {{ haproxy_backend_servers|length }}
    # Patroni has an active leader and inactive replicas
    num_servers_wanted: >
      {{ (failover_manager == 'patroni')|ternary(1, num_servers) }}
    up_servers: >
      {{ servers|map(attribute='status')|reject('equalto', 'DOWN')|list|length }}
    servers: >-
      {{ haproxy_stats|selectattr('pxname', 'equalto', 'be')|rejectattr('svname', 'equalto', 'BACKEND')|list }}

- name: Ensure each server in the read-only backend is up
  assert:
    msg: >-
      Expected {{ num_servers_wanted }} servers to be UP, got {{ up_servers }} instead
      {% for s in servers %}{{ s.svname }}={{ s.status }}{{ loop.last|ternary('', ', ') }}{% endfor %}
    that:
      - up_servers == num_servers_wanted
  vars:
    num_servers: >
      {{ haproxy_backend_servers|length }}
    # Patroni has an inactive leader and active replicas
    num_servers_wanted: >
      {{ (failover_manager == 'patroni')|ternary(num_servers|int - 1, num_servers) }}
    up_servers: >
      {{ servers|map(attribute='status')|reject('equalto', 'DOWN')|list|length }}
    servers: >-
      {{ haproxy_stats|selectattr('pxname', 'equalto', 'be_ro')|rejectattr('svname', 'equalto', 'BACKEND')|list }}
  when:
    haproxy_read_only_load_balancer_enabled|default(false)
