---

# Â© Copyright EnterpriseDB UK Limited 2015-2025 - All rights reserved.

- name: Set tpa_dir
  set_fact:
    tpa_dir: "{{ tpa_dir|default(lookup('env', 'TPA_DIR')) }}"

- name: Turn included_tasks into a list if required
  set_fact:
    included_tasks_list: "{{ included_tasks|default('')|replace(',', ' ')|split }}"
  when: included_tasks|default('') is string

- name: Turn excluded_tasks into a list if required
  set_fact:
    excluded_tasks_list: "{{ excluded_tasks|default('')|replace(',', ' ')|split }}"
  when: excluded_tasks|default('') is string

- name: Set up selectors for tag checks
  set_fact:
    task_selector:
      include: "{{ included_tasks_list|default(included_tasks) }}"
      exclude: "{{ excluded_tasks_list|default(excluded_tasks) }}"

# Normally, bin/tpaexec will set cluster_dir to the cluster directory we
# are using. That doesn't happen when we're running under Ansible Tower,
# so we try to derive the location from the project directory.

- name: Set cluster_dir to Tower project directory, if available
  vars:
    awx_data_dir: "{{ lookup('env', 'AWX_PRIVATE_DATA_DIR') }}"
  when:
    cluster_dir is not defined and awx_data_dir != ''
  block:
  - stat:
      path: "{{ awx_data_dir ~ '/project' }}"
    register: project_dir
    delegate_to: localhost
  - assert:
      that: project_dir.stat.exists
      fail_msg: project directory not found in {{ awx_data_dir }}
  - set_fact:
      cluster_dir: "{{ project_dir.stat.path }}"

# We set up the ssh configuration here. Also, regardless of platform, we
# wait until ssh connections succeed (or clearly don't work, e.g., due
# to authentication failures).

- include_tasks: ssh.yml
  when:
    ansible_connection == 'ssh'

# If we need to do anything platform-specific, now's our chance. (For
# example, on AWS EC2 instances, we translate some dynamic inventory
# settings and wait for instance reachability checks to pass.)

- include_role:
    name: "init/platforms/{{ platform }}"
  vars:
    platform_tasks_main:
      "{{ tpa_dir }}/roles/init/platforms/{{ platform }}/tasks/main.yml"
  when:
    platform_tasks_main is file

# Next up are tasks that apply to instances regardless of platform,
# including implicit role setting and validation, hostname and alias
# enumeration, etc.

- include_tasks: roles.yml
- include_tasks: hostvars.yml
- include_tasks: volumes.yml
- include_tasks: sys.yml
- include_tasks: ssl.yml

# Detect which distribution is in use, and fail if it's not supported.
# (We use the minimal_setup module because we can't rely on a usable
# Python interpreter being available yet.)

- include_tasks: distribution.yml

- include_tasks: locale.yml

# Set various facts related to Postgres and other components we need to
# install and configure (BDR, pgbouncer, haproxy, etc.). Also complains
# about various configuration problems.

- include_tasks: postgres.yml
- include_tasks: src.yml
- include_role: name=haproxy/facts
  when: >
    'haproxy' in role

- include_role: name=patroni/facts

# Decide which repositories we should use, and check that we have
# whatever credentials we need to access them.

- include_tasks: repos.yml

# We attempt to gather postgres facts only if there is a valid PGDATA
# directory, the postgres user exists, and postgres is running; or in
# other words, only if we have a reasonable chance of being able to
# connect to Postgres. At this early stage of deployment, it is not an
# error if these conditions are not met (e.g., when we are attaching an
# existing PGDATA to a new instance).

- include_role: name=postgres/facts
  when: >
    'postgres' in role
    and pgdata_initialised|default(False)
    and postgres_running|default(False)
    and pgdata_user is defined

# We may change default values over time, but the changes are not always
# worth forcing a reload or restart on existing clusters. Now that we've
# finished running minimal_postgres_setup and cluster_discovery, we can
# alter default values, if needed, based on the values in cluster_facts.

- include_tasks: update-defaults.yml
