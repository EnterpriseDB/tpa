---

# Copyright Â© 2ndQuadrant Limited <info@2ndquadrant.com>

- include_role: name=postgres/createuser
  vars:
    username: repmgr
    role_attrs: [replication]

# Determine the upstream primary for this host, that is, the primary at
# the root of the cascading replication setup.

- name: Record name of the upstream primary
  set_fact:
    upstream_primary: "{{ inventory_hostname|upstream_root(hostvars) }}"

# Find any backed-up instance that is, or is a replica of, said primary.
# We would prefer to use a backup server in the same region, but we'll
# take whatever we can find.

- name: Record name of any upstream or sibling with backups
  set_fact:
    upstream_backedup: "{{
      groups[cluster_tag]
      |instance_with_backup_of(upstream_primary, inventory_hostname, hostvars)
    }}"

- name: Set default repmgr_location
  set_fact:
    repmgr_location: "{{ repmgr_location|default(ec2_availability_zone|default('')) }}"

- name: Ensure repmgr directories exist
  file:
    path: "{{ item }}"
    owner: "{{ postgres_user }}"
    group: "{{ postgres_group }}"
    mode: 0755
    state: directory
  with_items:
    - "{{ repmgr_conf_dir }}"

- name: Install repmgr.conf
  template:
    src: "{{ item.path }}.j2"
    dest: "{{ repmgr_conf_dir }}/{{ item.path }}"
    owner: "{{ postgres_user }}"
    group: "{{ postgres_group }}"
    mode: "{{ item.mode }}"
  with_items:
    - {mode: "0644", path: repmgr.conf}
  notify:
    - Note repmgrd restart required

- name: Install additional repmgr scripts
  template:
    src: "{{ item.path }}.j2"
    dest: "{{ repmgr_conf_dir }}/{{ item.path }}"
    owner: "{{ postgres_user }}"
    group: "{{ postgres_group }}"
    mode: "{{ item.mode }}"
  with_items:
    - {mode: "0755", path: promote_command.sh}
    - {mode: "0755", path: follow_command.sh}

- name: Set up a cron job to purge old monitor records
  cron:
    user: "{{ postgres_user }}"
    cron_file: /etc/cron.d/repmgr_cleanup
    name: Purge old repmgr monitor records
    minute:  "{{ _repmgr_cluster_cleanup_interval[0] }}"
    hour:    "{{ _repmgr_cluster_cleanup_interval[1] }}"
    day:     "{{ _repmgr_cluster_cleanup_interval[2] }}"
    month:   "{{ _repmgr_cluster_cleanup_interval[3] }}"
    weekday: "{{ _repmgr_cluster_cleanup_interval[4] }}"
    job: >
      repmgr -f "{{ repmgr_conf_file }}" node status | grep -q 'Role: primary' &&
      repmgr -f "{{ repmgr_conf_file }}" cluster cleanup --keep-history 7 2> /dev/null
    state: present
