---

# Â© Copyright EnterpriseDB UK Limited 2015-2025 - All rights reserved.

# We want to set this server into the MAINT state in haproxy, which
# disables health checks and prevents new connections to the server
# (it does not terminate existing connections, as the documentation
# indirectly suggests). This is equivalent to "disable server".
#
# We would also like to know if the server was active, so that we can
# take some additional steps below. Unfortunately, there is no way to
# do this in a single step.
#
# We can tell if haproxy is directing traffic to a server by looking
# for an entry with server_id=NNN in the stick table for the backend
# ("show table be"). To make sense of the server_ids, we need to look
# at the server state ("show servers state be"). If we then switch to
# MAINT, we can parse the output to figure out if the server had been
# receiving traffic before the switch.
#
# We assume that pgbouncer is connecting to haproxy on the same host,
# so we send "show table be key 127.0.0.1" to find only the relevant
# entry in the stick table.
#
# This is not resilient against changes to the traffic routing because
# of failed health checks during the process. The best we can do is to
# minimise the window of opportunity for problems by issuing all three
# commands in one go.
#
# Note: I have not been able to find any advantage to using the DRAIN
# state in haproxy, or even understand exactly what it does. However,
# it does not remove the stick table entry for the server, so we have
# to issue an additional "clear table be key 127.0.0.1".
#
# Of course, we have to do this for every proxy in my_haproxies.

- name: Put backend {{ inventory_hostname }} in MAINT mode
  shell: >
    echo "show table be key 127.0.0.1;
    show servers state be;
    set server be/{{ inventory_hostname }} state maint" |
    socat stdio "{{ haproxy_stats_socket }}"
  args:
    executable: /bin/bash
  register: haproxy_stats
  with_items: "{{ my_haproxies }}"
  delegate_to: "{{ item }}"

# Now we parse the status lines from each haproxy to set
# `last_active_backend_for_proxy[haproxy_instance]`.

- include_tasks: parse-haproxy-status.yml
  vars:
    haproxy_instance: "{{ item.0 }}"
    haproxy_status:
      "{{ item.1.stdout_lines|default([]) }}"
  with_together:
  - "{{ my_haproxies }}"
  - "{{ haproxy_stats.results }}"

# If this was an active backend, we must ask pgbouncer to reconnect to
# haproxy (which will now direct it to a different backend server) and
# wait for all active client connections to be closed.
#
# We assume that every haproxy instance also runs a pgbouncer pointing
# to the haproxy (which is true of BDR-Always-ON by default).

- name: Issue RECONNECT{{ wait_close|default('yes')|bool|ternary(' and WAIT_CLOSE ', ' ') }}to pgbouncer
  postgresql_query:
    conninfo: "{{ _dsn|dbname('pgbouncer', user='pgbouncer') }}"
    autocommit: yes
    queries: "{{
        [_commands[0]]|union(
          wait_close|default('yes')|bool|ternary([_commands[1]], [])
        )
      }}"
  vars:
    _dsn: "{{ hostvars[haproxy_instance].pgbouncer_node_dsn }}"
    _task_environment:
      PGPASSWORD: "{{ pgbouncer_password }}"
    _commands:
      - text: RECONNECT
      - text: WAIT_CLOSE
  environment: "{{ target_environment|combine(_task_environment) }}"
  with_items: "{{ my_haproxies }}"
  loop_control:
    loop_var: haproxy_instance
  delegate_to: "{{ haproxy_instance }}"
  when:
    inventory_hostname == last_active_backend_for_proxy[haproxy_instance]

# Once the server is guaranteed to have no active traffic, we can wait
# to ensure that any writes it originated have finished replicating to
# the other backends.

- name: Wait for replication to complete
  postgresql_query:
    conninfo: "{{ dsn|dbname(bdr_database) }}"
    query: >
      select bdr.wait_slot_confirm_lsn(NULL, NULL)
  become_user: "{{ postgres_user }}"
  become: yes
  when: >
    'bdr' in role
