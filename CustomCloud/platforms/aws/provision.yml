---

# Provision EC2 instances according to a separate configuration file.
#
# The appropriate AWS IAM credentials must be supplied separately, e.g.
# in ~/.boto or in the environment.

- name: Provision AWS cluster
  hosts: localhost

  # The configuration comes from an external file, but we derive some
  # sensible defaults for internal use. We must use different variable
  # names to avoid template recursion when the values are overriden.

  vars:
    ec2_key_file: "{{ key_file | default('id_' + cluster_name|lower) }}"
    ec2_key_name: "{{ key_name | default('2Q_' + cluster_name|lower) }}"
    variables: {}

  tasks:

    # Make sure we're invoked with a complete cluster configuration. We
    # expect $cluster_dir/$config.yml to define a cluster name and a
    # list of instances.

    - include: validate.yml
      tags: always

    # Create an ssh keypair for this cluster, and register its public
    # key in each region. This key is used to initially provide ssh
    # access to any provisioned instances.

    - name: Create ssh key {{ ec2_key_file }}
      include: ../common/ssh-key.yml
      vars:
        keyfile: "{{ ec2_key_file }}"
      tags: ec2_keys

    - name: Register key {{ ec2_key_name }} in each region
      ec2_key:
        state: present
        region: "{{ item }}"
        name: "{{ ec2_key_name }}"
        key_material: "{{ lookup('file', cluster_dir + '/' + ec2_key_file + '.pub') }}"
        wait: yes
      with_items: "{{ regions }}"
      tags: ec2_keys

    # When provisioning each instance, we must specify an AMI id to use.
    # If the instance configuration specifies "image: ami-xxx" directly,
    # we can use that value. As a convenience, if ec2_ami_name is set at
    # the top level, we search for a matching AMI in each region and use
    # its id by default.
    #
    # The AMI for a region is then given by ec2_region_amis[region].

    - name: Initialise empty region→AMI table
      set_fact:
        ec2_region_amis: {}

    - block:
        - name: Find AMI named {{ ec2_ami_name }} in each region
          ec2_ami_find:
            name: "{{ ec2_ami_name }}"
            owner: "{{ ec2_ami_owner|default(omit) }}"
            region: "{{ item }}"
            ami_tags: "{{ ec2_ami_tags|default(omit) }}"
            sort: name
            sort_order: descending
            sort_end: 1
            no_result_action: fail
          register: ec2_amis
          with_items: "{{ regions }}"

        - name: Initialise region→AMI table
          set_fact:
            ec2_region_amis: "{{ ec2_region_amis|combine({item.0: item.1.results.0.ami_id}) }}"
          with_together:
            - regions
            - ec2_amis.results
      when: ec2_ami_name is defined

    # When provisioning each instance, we must specify a vpc_subnet_id
    # to place the instance in. If the instance configuration specifies
    # "subnet: subnet-xxx" directly, we can use that value. This allows
    # us to support existing complex network configurations, but is not
    # especially convenient to use.
    #
    # If there's no existing network to integrate with, we can set up a
    # simple network configuration.
    #
    # We limit cluster operations to a single VPC, which is defined by
    # setting ec2_vpc at the top level:
    #
    #   ec2_vpc:
    #     Name: ExampleVPC
    #     cidr: 192.0.2.0/24
    #
    # This expression must uniquely identify a VPC in each region by its
    # name and cidr. If the VPC does not exist, it will be created (and
    # this cluster will be marked as its creator).
    #
    # The vpc-id for a region is then given by ec2_region_vpcs[region].

    - name: Resolve ec2_vpc specification
      include: vpc.yml
      vars:
        vpc: "{{ ec2_vpc }}"
        region: "{{ r }}"
      with_items: "{{ regions }}"
      loop_control:
        loop_var: r

    # Now that we know which VPC to use in each region, we try to make
    # sense of the subnet references in each instance definition. If a
    # CIDR corresponds to an existing subnet within the right VPC, we
    # just use its subnet id.
    #
    # If not, we create the subnet; but to do so, we need to know what
    # availability zone to create it in. This is specified by setting
    # ec2_vpc_subnets so that ec2_vpc_subnets[region][cidr] gives the
    # desired availability zone:
    #
    #   ec2_vpc_subnets:
    #     us-east-1:
    #       192.0.2.0/27: us-east-1b
    #       192.0.2.100/27: us-east-1b
    #     us-west-2:
    #       192.0.2.200/27: us-west-2c
    #       …
    #
    # Once all subnet references have been resolved, the subnet-id for a
    # subnet in a region is given by ec2_region_subnets[region][cidr].

    - name: Resolve subnet references in each region
      include: vpc-subnet.yml
      vars:
        region: "{{ r }}"
        vpc: "{{ ec2_region_vpcs[r] }}"
        subnets: "{{
          instances|selectattr('region','equalto',r)|map(attribute='subnet')|unique|list
        }}"
      with_items: "{{ regions }}"
      loop_control:
        loop_var: r

    # A security group is a set of firewall rules that can be applied to
    # instances in a VPC. (A subnet in a VPC can also have a network ACL
    # associated with it, but we use the default that allows all inbound
    # and outbound traffic. Ansible can't operate on ACLs anyway.)
    #
    # http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html
    # http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html
    #
    # We create a security group for every VPC we are using.

    - name: Set default security group rules for the cluster
      set_fact:
        cluster_rules:
          - {proto: tcp, from_port: 22, to_port: 22, cidr_ip: 0.0.0.0/0}
          - {proto: tcp, from_port: 5432, to_port: 5432, cidr_ip: 0.0.0.0/0}
          - {proto: tcp, from_port: 1194, to_port: 1194, cidr_ip: 0.0.0.0/0}
      when: cluster_rules is not defined

    - name: Create security groups for each VPC
      ec2_group:
        state: present
        region: "{{ item.key }}"
        vpc_id: "{{ item.value }}"
        name: "Group {{ cluster_name }}/{{ item.key }}"
        description: "Automatically created for {{ cluster_name }} in {{ item.key }}"
        rules: "{{ cluster_rules }}"
      with_dict: "{{ ec2_region_vpcs }}"
      register: sgs

    - name: Record security group ids in each region
      set_fact:
        ec2_region_groups: "{{
          ec2_region_groups|default({})|combine({
            item.0: item.1.group_id
          })
        }}"
      with_together:
        - "{{ regions }}"
        - "{{ sgs.results }}"

    # Create an "instance profile" (== an IAM role) for the cluster, and
    # attach an inline policy with any permissions that the instances
    # need (which is only associate-address now; when we need other
    # rights, we should move the policy into a separate file).

    - name: Create an instance profile for the cluster
      iam:
        iam_type: role
        name: "{{ cluster_profile }}"
        state: present
      register: profile
      tags: iam

    - name: Record instance profile ARN for later use
      include: ../common/set-cluster-var.yml
      vars:
        name: instance_profile_arn
        value: "{{ profile.instance_profile_result.arn }}"
      when: profile|changed
      tags: iam

    - name: Attach a policy to the instance profile
      iam_policy:
        iam_type: role
        iam_name: '{{ cluster_profile }}'
        policy_name: "{{ cluster_name }}_instance_permissions"
        state: present
        policy_json: |
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Action": [
                  "ec2:DescribeAddresses",
                  "ec2:DisassociateAddress",
                  "ec2:AssociateAddress"
                ],
                "Effect": "Allow",
                "Resource": "*"
              }
            ]
          }

    # We start with the array of instances defined in config.yml, some
    # of whose entries may have instance_count > 1. We expand the array
    # by replacing each such entry with instance_count identical copies.
    #
    # We want stable node numbers (because they're used in count_tags
    # and to assign IP addresses, among other things), so it's not safe
    # to change instance_counts after provisioning. Entries should only
    # ever be appended to instances[], even if that means duplicating an
    # earlier entry. Here's a dire uppercased warning to that effect:
    #
    # !!! DON'T CHANGE instance_count AFTER PROVISIONING !!!

    - name: Expand the array of EC2 instances
      set_fact:
        instances: "{{ instances|expand_instances }}"

    # Create EC2 instances using the VPC subnets and security groups and
    # access key we configured above. We loop over instances[], look up
    # the VPC and group corresponding to the region/subnet defined for
    # that instance, and extract the relevant subnet or group id from
    # the responses registered above.
    #
    # Using "volumes" as we do here means that we can't tag the volumes
    # that we create; if delete_on_termination isn't set, that means we
    # can't find them later to clean them up. We should use ec2_vol to
    # create tagged volumes and attach them to the instances. Then we
    # can use ec2_vol_facts to find them later.

    - name: Set up EC2 instances
      ec2:
        exact_count: 1
        count_tag: "{{ cluster_tags|combine(item.tags) }}"
        image: "{{ item.image|default(ec2_region_amis[item.region]) }}"
        region: "{{ item.region }}"
        key_name: "{{ ec2_key_name }}"
        instance_type: "{{ item.type }}"
        instance_tags: "{{ cluster_tags|combine(item.tags) }}"
        instance_profile_name: "{{ cluster_profile }}"
        private_ip: "{{ item.private_ip|default(omit) }}"
        vpc_subnet_id: "{{ ec2_region_subnets[item.region][item.subnet] }}"
        group_id: "{{ ec2_region_groups[item.region] }}"
        volumes: "{{ item.volumes|default(omit) }}"
        spot_price: "{{ item.spot_price|default(omit) }}"
        spot_wait_timeout: "{{ item.spot_wait_timeout|default(omit) }}"
        spot_launch_group: "{{ item.spot_launch_group|default(omit) }}"
        user_data: "{{ lookup('template', 'user-data.j2') }}"
        termination_protection: "{{ item.termination_protection|default('no') }}"
        assign_public_ip: yes
        wait: yes
      with_items: "{{ instances }}"
      register: ec2_instances
      async: 7200
      poll: 0
      tags: ec2

    - name: Wait for instance provisioning to complete
      async_status: jid={{ item.ansible_job_id }}
      register: ec2_jobs
      until: ec2_jobs.finished
      retries: 300
      with_items: "{{ ec2_instances.results }}"

    # If explicitly requested, we can also associate an elastic IP with
    # one or more of the instances created above. The instance ids come
    # from the results above, and the configuration is in instances[].

    - name: Associate elastic IPs with instances
      ec2_eip:
        state: present
        region: "{{ item.1.tagged_instances[0].region }}"
        device_id: "{{ item.1.tagged_instances[0].id }}"
        reuse_existing_ip_allowed: true
        in_vpc: true
      when: item.0.assign_eip|d() and item.0.assign_eip
      with_together:
        - instances
        - ec2_jobs.results
      register: ec2_eips
      tags: ec2_eips

    # Now we have ec2_public_ips in ec2_jobs.results, but some of them
    # may have been overriden by elastic IP addresses.

    - name: Set instance variables and elastic IP overrides
      set_fact:
        instance_vars: "{{
          instance_vars|default([])|union([
            item.0.tagged_instances[0]|combine({
              'public_ip': item.1.public_ip|default(item.0.tagged_instances[0].public_ip)
            })
          ])
        }}"
      with_together:
        - ec2_jobs.results
        - ec2_eips.results

    # Write a static inventory file for the newly-provisioned hosts into
    # the cluster's inventory directory.

    - name: Ensure that the cluster's inventory directory exists
      file:
        path: "{{ cluster_dir }}/inventory"
        state: directory

    - name: Write a static inventory file for the cluster
      template:
        src: inventory.j2
        dest: "{{ cluster_dir }}/inventory/00-{{ cluster_name }}"

    - name: Copy ec2.py to inventory directory
      copy:
        src: ec2.py
        dest: "{{ cluster_dir }}/inventory/ec2.py"
        mode: "0755"

    - name: Install customised ec2.ini
      template:
        src: ec2.ini.j2
        dest: "{{ cluster_dir }}/inventory/ec2.ini"

    - include: ../common/write-cluster-vars.yml
