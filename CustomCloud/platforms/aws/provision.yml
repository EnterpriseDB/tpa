---

# Provision EC2 instances according to a separate configuration file.
#
# The appropriate AWS IAM credentials must be supplied separately, e.g.
# in ~/.boto or in the environment.

- include: ../common/provision.yml
  tags: common

- name: Provision AWS cluster
  hosts: localhost

  # The configuration comes from an external file, but we derive some
  # sensible defaults for internal use. We must use different variable
  # names to avoid template recursion when the values are overriden.

  vars:
    ec2_key_file: "{{ key_file | default('id_' + cluster_name|lower) }}"
    ec2_key_name: "{{ key_name | default('2Q_' + cluster_name|lower) }}"
    platform_vars: {}

  tasks:

    # Make sure we're invoked with a complete cluster configuration. We
    # expect $cluster_dir/$config.yml to define a cluster name and a
    # list of instances.

    - include: validate.yml
      tags: always

    - name: Copy ec2.py to inventory directory
      copy:
        src: ec2.py
        dest: "{{ cluster_dir }}/inventory/ec2.py"
        mode: "0755"

    - name: Install customised ec2.ini
      template:
        src: ec2.ini.j2
        dest: "{{ cluster_dir }}/inventory/ec2.ini"

    # Register this cluster's ssh public key in each region. This key is
    # used to initially provide ssh access to any provisioned instances.

    - name: Register key {{ ec2_key_name }} in each region
      ec2_key:
        state: present
        region: "{{ item }}"
        name: "{{ ec2_key_name }}"
        key_material: "{{ lookup('file', cluster_dir + '/' + ec2_key_file + '.pub') }}"
        wait: yes
      with_items: "{{ regions }}"
      tags: ec2_keys

    # When provisioning each instance, we must specify a vpc_subnet_id
    # to place the instance in. If the instance configuration specifies
    # "subnet: subnet-xxx" directly, we can use that value. This allows
    # us to support existing complex network configurations, but is not
    # especially convenient to use.
    #
    # If there's no existing network to integrate with, we can set up a
    # simple network configuration.
    #
    # To begin with, we must specify the VPC to use. This can be done in
    # many different ways.
    #
    #   # Every region must have a VPC named ExampleVPC.
    #   ec2_vpc:
    #     Name: ExampleVPC
    #
    #   # Every region must have a VPC named ExampleVPC with a matching
    #   # CIDR; if it does not exist, we will try to create it.
    #   ec2_vpc:
    #     Name: ExampleVPC
    #     cidr: 192.0.2.0/24
    #
    #   # In order to use different VPCs in each region, for example to
    #   # specify VPCs by id, this expanded form maps from region names
    #   # to a VPC filter specification. If the VPC does not exist, and
    #   # both Name and cidr are given (and vpc-id is not in filters),
    #   # we will try to create it.
    #   ec2_vpc:
    #     us-east-1:
    #       Name: ExampleVPC
    #       cidr: 192.0.2.0/24
    #       filters:
    #         vpc-id: vpc-abcdef
    #
    # In each case, ec2_vpc must uniquely identify a VPC in each region.
    # If Name/cidr are specified, we will attempt to create the VPC if
    # it does not exist (and mark this cluster as its creator).
    #
    # The vpc-id for a region is then given by ec2_region_vpcs[region].

    - name: Resolve ec2_vpc specification
      include: vpc.yml
      vars:
        vpc: "{{ ec2_vpc[r]|default(ec2_vpc) }}"
        region: "{{ r }}"
      with_items: "{{ regions }}"
      loop_control:
        loop_var: r
      tags: ec2_vpc

    # Now that we know which VPC to use in each region, we try to make
    # sense of the subnet references in each instance definition. If a
    # CIDR corresponds to an existing subnet within the right VPC, we
    # just use its subnet id.
    #
    # If not, we create the subnet; but to do so, we need to know what
    # availability zone to create it in. This is specified by setting
    # ec2_vpc_subnets so that ec2_vpc_subnets[region][cidr] gives the
    # desired availability zone:
    #
    #   ec2_vpc_subnets:
    #     us-east-1:
    #       192.0.2.0/27: us-east-1b
    #       192.0.2.100/27: us-east-1b
    #     us-west-2:
    #       192.0.2.200/27: us-west-2c
    #       â€¦
    #
    # Once all subnet references have been resolved, the subnet-id for a
    # subnet in a region is given by ec2_region_subnets[region][cidr].

    - name: Resolve subnet references in each region
      include: vpc-subnet.yml
      vars:
        region: "{{ r }}"
        vpc: "{{ ec2_region_vpcs[r] }}"
        subnets: "{{
          instances|selectattr('region','equalto',r)|map(attribute='subnet')|unique|list
        }}"
      with_items: "{{ regions }}"
      loop_control:
        loop_var: r
      tags: ec2_vpc

    # A security group is a set of firewall rules that can be applied to
    # instances in a VPC. (A subnet in a VPC can also have a network ACL
    # associated with it, but we use the default that allows all inbound
    # and outbound traffic. Ansible can't operate on ACLs anyway.)
    #
    # http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html
    # http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html
    #
    # To use an existing security group, specify it as follows:
    #
    # ec2_groups:
    #   us-east-1:
    #     group-name: foo
    #
    # That is, ec2_groups maps region names to a filter expression that
    # uniquely identifies a security group in any of the ways documented
    # at http://docs.ansible.com/ansible/ec2_group_facts_module.html and
    # https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeSecurityGroups.html
    #
    # Existing security groups are assumed to contain all required rules
    # already. If no security group is specified, we create one for each
    # VPC with the correct rules.

    - name: Set default security group rules for the cluster
      set_fact:
        cluster_rules:
          - {proto: tcp, from_port: 22, to_port: 22, cidr_ip: 0.0.0.0/0}
          - {proto: udp, from_port: 1194, to_port: 1194, cidr_ip: 0.0.0.0/0}
      when: cluster_rules is not defined
      tags: ec2_vpc

    - name: Initialise empty ec2_region_groups
      set_fact:
        ec2_region_groups: {}

    # Find the security group corresponding to each entry in ec2_groups
    # and add entries to ec2_region_groups.

    - name: Find existing security groups
      ec2_group_facts:
        region: "{{ item.key }}"
        filters: "{{ item.value }}"
      with_dict: "{{ ec2_groups|default({}) }}"
      register: sgs
      tags: ec2_vpc

    - name: Extend ec2_region_groups
      set_fact:
        ec2_region_groups: "{{
          ec2_region_groups|combine({
            item.item.key: item|json_query('security_groups[*].group_id')
          })
        }}"
      with_items: "{{ sgs.results }}"
      tags: ec2_vpc

    - name: Create security groups for each VPC
      ec2_group:
        state: present
        region: "{{ item.key }}"
        vpc_id: "{{ item.value }}"
        name: "Group {{ cluster_name }}/{{ item.key }}"
        description: "Automatically created for {{ cluster_name }} in {{ item.key }}"
        rules: "{{ cluster_rules }}"
      when:
        item.key not in ec2_region_groups
      with_dict: "{{ ec2_region_vpcs }}"
      register: new_sgs
      tags: ec2_vpc

    - name: Record security group ids in each region
      set_fact:
        ec2_region_groups: "{{
          ec2_region_groups|default({})|combine({
            item.0: item.1.group_id
          })
        }}"
      when:
        ec2_region_groups[item.0] is not defined and
        item.1.group_id is defined
      with_together:
        - "{{ regions }}"
        - "{{ new_sgs.results }}"
      tags: ec2_vpc

    # Create an "instance profile" IAM role for the cluster, and attach
    # an inline policy with any permissions that the instances need.

    - name: Create an instance profile for the cluster
      iam:
        iam_type: role
        name: "{{ cluster_profile }}"
        state: present
      register: profile
      tags: iam

    - name: Attach a policy to the instance profile
      iam_policy:
        iam_type: role
        iam_name: '{{ cluster_profile }}'
        policy_name: "{{ cluster_name }}_instance_permissions"
        state: present
        policy_json: |
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Action": [
                  "ec2:DescribeAddresses",
                  "ec2:DisassociateAddress",
                  "ec2:AssociateAddress"
                ],
                "Effect": "Allow",
                "Resource": "*"
              },
              {
                "Action": [
                  "s3:GetObject"
                ],
                "Effect": "Allow",
                "Resource": "arn:aws:s3:::{{ cluster_bucket }}/{{ cluster_name }}/*"
              }
            ]
          }
      tags: iam

    # We generated host keys into hostkeys/ssh_host_$type_key{,.pub};
    # we now upload those to hostkeys/$type{,.pub}.txt in S3.

    - name: Upload SSH host key files
      s3:
        bucket: "{{ cluster_bucket }}"
        object: "{{ cluster_name }}/hostkeys/{{ item.0 }}{{ item.1 }}.txt"
        src: "{{ cluster_dir }}/hostkeys/ssh_host_{{ item.0 }}_key{{ item.1 }}"
        overwrite: different
        expiration: 3600
        mode: put
      with_nested:
        - ["rsa", "ecdsa"]
        - ["", ".pub"]
      register: keyurls
      tags: ssh

    # When provisioning each instance, we must specify an AMI id to use.
    # If the instance configuration specifies "image: ami-xxx" directly,
    # we can use that value. As a convenience, if ec2_ami is set at the
    # top level, we search for a matching AMI in each region and use its
    # id by default.
    #
    # The AMI for a region is then given by ec2_region_amis[region].
    #
    # We also record the properties of each AMI id for later use.

    - name: Initialise empty regionâ†’AMI table
      set_fact:
        ec2_region_amis: {}
        ec2_ami_properties: {}
      tags: ec2

    - block:
        - name: Find AMI named {{ ec2_ami['Name'] }} in each region
          ec2_ami_find:
            name: "{{ ec2_ami['Name'] }}"
            owner: "{{ ec2_ami['Owner']|default(omit) }}"
            ami_tags: "{{ ec2_ami['tags']|default(omit) }}"
            region: "{{ item }}"
            sort: name
            sort_order: descending
            sort_end: 1
            no_result_action: fail
          with_items: "{{ regions }}"
          register: ec2_amis

        - name: Initialise regionâ†’AMI table
          set_fact:
            ec2_region_amis: "{{ ec2_region_amis|combine({item.0: item.1.results.0.ami_id}) }}"
            ec2_ami_properties: "{{ ec2_ami_properties|combine({item.1.results.0.ami_id: item.1.results.0}) }}"
          with_together:
            - "{{ regions }}"
            - "{{ ec2_amis.results }}"
      when: ec2_ami is defined
      tags: ec2

    # We also need to retrieve the properties for any AMIs specified
    # directly by id.

    - name: Look for AMIs specified directly by id
      ec2_ami_find:
        ami_id: "{{ item.image }}"
        region: "{{ item.region }}"
        no_result_action: fail
      with_items: "{{ instances|select('has_subkey','image')|list }}"
      register: ec2_direct_amis
      tags: ec2

    - name: Extend AMIâ†’properties table
      set_fact:
        ec2_ami_properties: "{{
            ec2_ami_properties|combine({item.results.0.ami_id: item.results.0})
        }}"
      with_items: "{{ ec2_direct_amis.results }}"
      tags: ec2

    # Preprocess instance definitions through various helpful filters to
    # set things that aren't set and check things that need checking.

    - name: Preprocess instance definitions
      set_fact:
        instances: "{{
          instances
          |expand_instance_tags(cluster_name)
          |expand_instance_image(ec2_region_amis)
          |expand_instance_volumes(ec2_ami_properties)
        }}"
      tags: ec2

    - name: Ensure that we know security group ids in every region
      assert:
        msg: "Group reference undefined"
        that:
          - ec2_region_groups[item.region] is defined
      with_items: "{{ instances }}"

    - name: Ensure that we know subnet ids in every region
      assert:
        msg: "Subnet reference undefined"
        that:
          - ec2_region_subnets[item.region] is defined
          - ec2_region_subnets[item.region][item.subnet] is defined
      with_items: "{{ instances }}"

    # Create EC2 instances using the VPC subnets and security groups and
    # access key we configured above. We loop over instances[], look up
    # the VPC and group corresponding to the region/subnet defined for
    # that instance, and extract the relevant subnet or group id from
    # the responses registered above.
    #
    # Using "volumes" as we do here means that we can't tag the volumes
    # that we create; if delete_on_termination isn't set, that means we
    # can't find them later to clean them up. We should use ec2_vol to
    # create tagged volumes and attach them to the instances. Then we
    # can use ec2_vol_facts to find them later.

    - name: Set up EC2 instances
      ec2:
        exact_count: 1
        count_tag: >
          {{
            cluster_tags|combine(item.tags)|combine({
              'Cluster': cluster_name,
              'Name': item.tags.Name,
              'role': (item.tags.role|join(",")),
              'node': item.node,
            })
          }}
        image: "{{ item.image }}"
        region: "{{ item.region }}"
        key_name: "{{ ec2_key_name }}"
        instance_type: "{{ item.type }}"
        instance_tags: >
          {{
            cluster_tags|combine(item.tags)|combine({
              'Cluster': cluster_name,
              'Name': item.tags.Name,
              'role': (item.tags.role|join(",")),
              'node': item.node,
            })
          }}
        instance_profile_name: "{{ cluster_profile }}"
        private_ip: "{{ item.private_ip|default(omit) }}"
        vpc_subnet_id: "{{ ec2_region_subnets[item.region][item.subnet] }}"
        group_id: "{{ ec2_region_groups[item.region] }}"
        volumes: "{{ item.volumes|default(omit) }}"
        spot_price: "{{ item.spot_price|default(omit) }}"
        spot_wait_timeout: "{{ item.spot_wait_timeout|default(omit) }}"
        spot_launch_group: "{{ item.spot_launch_group|default(omit) }}"
        user_data: "{{
          lookup('template', 'user-data.j2',
            vars=dict(image=ec2_ami_properties[item.image])
          )
        }}"
        termination_protection: "{{ item.termination_protection|default('no') }}"
        assign_public_ip: "{{ item.assign_public_ip|default('yes') }}"
        wait: yes
      with_items: "{{ instances }}"
      register: ec2_instances
      async: 7200
      poll: 0
      tags: ec2

    - name: Wait for instance provisioning to complete
      async_status: jid={{ item.ansible_job_id }}
      register: ec2_jobs
      until: ec2_jobs.finished
      retries: 300
      with_items: "{{ ec2_instances.results }}"
      tags: ec2

    # Tag all volumes attached to the instances we provisioned.

    - name: Tag volumes attached to instances
      ec2_tag:
        state: present
        region: "{{ item.1.region }}"
        resource: "{{ item.1.block_device_mapping[item.2].volume_id }}"
        tags: >
          {{
            cluster_tags|combine({
              "Name": cluster_name + ':' + item.1.tags.node + ':' + item.2,
              "CreatingCluster": cluster_name,
            })
          }}
      with_nested_dependents:
        - ec2_jobs.results
        - item.0.tagged_instances
        - item.1.block_device_mapping.keys()|list
      tags: ec2

    # If explicitly requested, we can also associate an elastic IP with
    # one or more of the instances created above. The instance ids come
    # from the results above, and the configuration is in instances[].

    - name: Associate elastic IPs with instances
      ec2_eip:
        state: present
        region: "{{ item.1.tagged_instances[0].region }}"
        device_id: "{{ item.1.tagged_instances[0].id }}"
        reuse_existing_ip_allowed: true
        in_vpc: true
      when: item.0.assign_elastic_ip|d() and item.0.assign_elastic_ip
      with_together:
        - "{{ instances }}"
        - "{{ ec2_jobs.results }}"
      register: ec2_eips
      tags: ec2_eips

    # Now we have ec2_public_ips in ec2_jobs.results, but some of them
    # may have been overriden by elastic IP addresses.

    - name: Set instance variables and elastic IP overrides
      set_fact:
        instance_vars: "{{
          instance_vars|default([])|union([
            item.0.tagged_instances[0]|combine({
              'ip_address': item.1.public_ip|default(item.0.tagged_instances[0].public_ip)
                or item.0.tagged_instances[0].private_ip,
              'public_ip': item.1.public_ip|default(item.0.tagged_instances[0].public_ip),
              'Name': item.0.item.item.tags|try_subkey('Name'),
              'role': item.0.item.item.tags|try_subkey('role'),
              'node': item.0.item.item.node,
            })
          ])
        }}"
      with_together:
        - "{{ ec2_jobs.results }}"
        - "{{ ec2_eips.results }}"

    # Write a static inventory file for the newly-provisioned hosts into
    # the cluster's inventory directory.

    - name: Write a static inventory file for the cluster
      template:
        src: inventory.j2
        dest: "{{ cluster_dir }}/inventory/00-{{ cluster_name }}"

    - name: Generate known_hosts file for the cluster
      template:
        src: known_hosts.j2
        dest: "{{ cluster_dir }}/known_hosts"

    - include: ../common/write-cluster-vars.yml
