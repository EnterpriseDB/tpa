---

# Provision EC2 instances and other necessary AWS resources (e.g., VPCs,
# security groups, subnets, etc.) based on the cluster definition file.
#
# The appropriate AWS IAM credentials must be supplied separately, e.g.
# in ~/.boto or in the environment.

- include: ../common/provision.yml
  tags: common

- name: Provision AWS cluster
  hosts: localhost
  tasks:
    # Make sure we're invoked with a complete cluster configuration. We
    # expect $cluster_dir/$config.yml to define a cluster name and a
    # list of instances.

    - include: validate.yml
      tags: always

    - name: Copy ec2.py to inventory directory
      copy:
        src: inventory/ec2.py
        dest: "{{ cluster_dir }}/inventory/ec2.py"
        mode: "0755"

    - name: Install customised ec2.ini
      template:
        src: inventory/ec2.ini.j2
        dest: "{{ cluster_dir }}/inventory/ec2.ini"

    - name: Remove old inventory cache
      file:
        path: "{{ cluster_dir }}/tmp/{{ item }}"
        state: absent
      with_items:
        - ansible-ec2.cache
        - ansible-ec2.index

    - include: ../common/read-provisioning-vars.yml

    # We register an SSH public key in each region to provide SSH access
    # to newly provisioned instances, unless ec2_key_name is explicitly
    # set to "" in config.yml.

    - include: ec2-register-key.yml
      when:
        ec2_instance_key is not defined and ec2_key_name != ''
      tags: ec2_keys

    # When provisioning each instance, we must specify a vpc_subnet_id
    # to place the instance in. If the instance configuration specifies
    # "subnet: subnet-xxx" directly, we can use that value. This allows
    # us to support existing complex network configurations, but is not
    # especially convenient to use.
    #
    # If there's no existing network to integrate with, we can set up a
    # simple network configuration.
    #
    # To begin with, we must specify the VPC to use. This can be done in
    # many different ways.
    #
    #   # Every region must have a VPC named ExampleVPC.
    #   ec2_vpc:
    #     Name: ExampleVPC
    #
    #   # Every region must have a VPC named ExampleVPC with a matching
    #   # CIDR; if it does not exist, we will try to create it.
    #   ec2_vpc:
    #     Name: ExampleVPC
    #     cidr: 192.0.2.0/24
    #
    #   # In order to use different VPCs in each region, for example to
    #   # specify VPCs by id, this expanded form maps from region names
    #   # to a VPC filter specification. If the VPC does not exist, and
    #   # both Name and cidr are given (and vpc-id is not in filters),
    #   # we will try to create it.
    #   ec2_vpc:
    #     us-east-1:
    #       Name: ExampleVPC
    #       cidr: 192.0.2.0/24
    #       filters:
    #         vpc-id: vpc-abcdef
    #
    # In each case, ec2_vpc must uniquely identify a VPC in each region.
    # If Name/cidr are specified, we will attempt to create the VPC if
    # it does not exist (and mark this cluster as its creator).
    #
    # The vpc-id for a region is then given by ec2_region_vpcs[region].

    - name: Resolve ec2_vpc specification
      include: vpc.yml
      vars:
        vpc: "{{ ec2_vpc[r]|default(ec2_vpc) }}"
        region: "{{ r }}"
      with_items: "{{ regions }}"
      loop_control:
        loop_var: r
      tags: ec2_vpc

    # Now that we know which VPC to use in each region, we try to make
    # sense of the subnet references in each instance definition. If a
    # CIDR corresponds to an existing subnet within the right VPC, we
    # just use its subnet id.
    #
    # If not, we create the subnet; but to do so, we need to know what
    # availability zone to create it in. This is specified by setting
    # ec2_vpc_subnets so that ec2_vpc_subnets[region][cidr] gives the
    # desired availability zone:
    #
    #   ec2_vpc_subnets:
    #     us-east-1:
    #       192.0.2.0/27:
    #         az: us-east-1b
    #       192.0.2.100/27:
    #         az: us-east-1b
    #     us-west-2:
    #       192.0.2.200/27:
    #         az: us-west-2c
    #       …
    #
    # Once all subnet references have been resolved, the subnet-id for a
    # subnet in a region is given by ec2_region_subnets[region][cidr].

    - name: Resolve subnet references in each region
      include: vpc-subnet.yml
      vars:
        region: "{{ r }}"
        vpc: "{{ ec2_region_vpcs[r] }}"
        subnets: "{{
          instances|selectattr('region','equalto',r)|map(attribute='subnet')|unique|list
        }}"
      with_items: "{{ regions }}"
      loop_control:
        loop_var: r
      tags: ec2_vpc

    # A security group is a set of firewall rules that can be applied to
    # instances in a VPC. (A subnet in a VPC can also have a network ACL
    # associated with it, but we use the default that allows all inbound
    # and outbound traffic. Ansible can't operate on ACLs anyway.)
    #
    # http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html
    # http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html
    #
    # To use an existing security group, specify it as follows:
    #
    # ec2_groups:
    #   us-east-1:
    #     group-name:
    #       - foo
    #       - bar
    #
    # That is, ec2_groups maps region names to a filter expression that
    # uniquely identifies a security group in any of the ways documented
    # at http://docs.ansible.com/ansible/ec2_group_facts_module.html and
    # https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeSecurityGroups.html
    #
    # Existing security groups are assumed to contain all required rules
    # already. If no security group is specified, we create one for each
    # VPC with the correct rules.

    - name: Set default security group rules for the cluster
      set_fact:
        custom_port: "{{ cluster_ssh_port|default(22)|int }}"
        cluster_rules:
          - {proto: tcp, from_port: 22, to_port: 22, cidr_ip: 0.0.0.0/0}
          - {proto: udp, from_port: 1194, to_port: 1194, cidr_ip: 0.0.0.0/0}
      when: cluster_rules is not defined
      tags: ec2_vpc

    - name: Add custom ssh port to security group
      set_fact:
        cluster_rules: "{{ cluster_rules|union([custom_rule]) }}"
      vars:
        custom_rule:
          proto: tcp
          from_port: "{{ custom_port }}"
          to_port: "{{ custom_port }}"
          cidr_ip: 0.0.0.0/0
      when: custom_port|int != 22

    - name: Initialise empty ec2_region_groups
      set_fact:
        ec2_region_groups: {}

    # Find the security group corresponding to each entry in ec2_groups
    # and add entries to ec2_region_groups.

    - name: Find existing security groups
      ec2_group_facts:
        region: "{{ item.key }}"
        filters: "{{ item.value }}"
      with_dict: "{{ ec2_groups|default({}) }}"
      register: sgs
      tags: ec2_vpc

    - name: Extend ec2_region_groups
      set_fact:
        ec2_region_groups: "{{
          ec2_region_groups|combine({
            item.item.key: item|json_query('security_groups[*].group_id')
          })
        }}"
      with_items: "{{ sgs.results }}"
      tags: ec2_vpc

    - name: Create security groups for each VPC
      ec2_group:
        state: present
        region: "{{ item.key }}"
        vpc_id: "{{ item.value }}"
        name: "Group {{ cluster_name }}/{{ item.key }}"
        description: "Automatically created for {{ cluster_name }} in {{ item.key }}"
        rules: "{{ cluster_rules }}"
      when:
        item.key not in ec2_region_groups
      with_dict: "{{ ec2_region_vpcs }}"
      register: new_sgs
      tags: ec2_vpc

    - name: Record security group ids in each region
      set_fact:
        ec2_region_groups: "{{
          ec2_region_groups|default({})|combine({
            item.0: item.1.group_id
          })
        }}"
      when:
        ec2_region_groups[item.0] is not defined and
        item.1.group_id is defined
      with_together:
        - "{{ regions }}"
        - "{{ new_sgs.results }}"
      tags: ec2_vpc

    # We create an instance profile IAM role for the cluster and attach
    # an inline policy with any permissions that the instances need,
    # unless cluster_profile is explicitly set to '' in config.yml.

    - include: iam-instance-profile.yml
      when:
        instance_profile_name is not defined and cluster_profile != ''
      tags: iam

    # We generated host keys into hostkeys/ssh_host_$type_key{,.pub};
    # we now upload those to hostkeys/$type{,.pub}.txt in S3.

    - name: Upload SSH host key files
      s3:
        bucket: "{{ cluster_bucket }}"
        object: "{{ cluster_name }}/hostkeys/{{ item.0 }}{{ item.1 }}.txt"
        src: "{{ cluster_dir }}/hostkeys/ssh_host_{{ item.0 }}_key{{ item.1 }}"
        overwrite: different
        expiration: 3600
        mode: put
      with_nested:
        - ["rsa", "ecdsa"]
        - ["", ".pub"]
      register: keyurls
      tags: ssh

    # When provisioning each instance, we must specify an AMI id to use.
    # If the instance configuration specifies "image: ami-xxx" directly,
    # we can use that value. As a convenience, if ec2_ami is set at the
    # top level, we search for a matching AMI in each region and use its
    # id by default.
    #
    # The AMI for a region is then given by ec2_region_amis[region].
    #
    # We also record the properties of each AMI id for later use.

    - name: Initialise empty mapping tables
      set_fact:
        ec2_volumes: {}
        ec2_region_amis: {}
        ec2_ami_properties: {}
      tags: ec2

    - block:
        - name: Find AMI named {{ ec2_ami['Name'] }} in each region
          ec2_ami_find:
            name: "{{ ec2_ami['Name'] }}"
            owner: "{{ ec2_ami['Owner']|default(omit) }}"
            ami_tags: "{{ ec2_ami['tags']|default(omit) }}"
            region: "{{ item }}"
            sort: name
            sort_order: descending
            sort_end: 1
            no_result_action: fail
          with_items: "{{ regions }}"
          register: ec2_amis

        - name: Initialise region→AMI table
          set_fact:
            ec2_region_amis: "{{ ec2_region_amis|combine({item.0: item.1.results.0.ami_id}) }}"
            ec2_ami_properties: "{{ ec2_ami_properties|combine({item.1.results.0.ami_id: item.1.results.0}) }}"
          with_together:
            - "{{ regions }}"
            - "{{ ec2_amis.results }}"
      when: ec2_ami is defined
      tags: ec2

    # We also need to retrieve the properties for any AMIs specified
    # directly by id.

    - name: Look for AMIs specified directly by id
      ec2_ami_find:
        ami_id: "{{ item.image }}"
        region: "{{ item.region }}"
        no_result_action: fail
      with_items: "{{ instances|select('has_subkey','image')|list }}"
      register: ec2_direct_amis
      tags: ec2

    - name: Extend AMI→properties table
      set_fact:
        ec2_ami_properties: "{{
            ec2_ami_properties|combine({item.results.0.ami_id: item.results.0})
        }}"
      with_items: "{{ ec2_direct_amis.results }}"
      tags: ec2

    # If «attach_existing» is specified for any volume in a region, we
    # build a table to map the names of unattached volumes in that
    # region to their volume-ids.

    - name: Look for existing attachable EBS volumes
      ec2_vol_facts:
        region: "{{ r }}"
        filters:
          status: available
          "tag:CreatingCluster": "{{ cluster_name }}"
      register: ec2_attachable_volumes
      with_items: "{{ regions }}"
      loop_control:
        loop_var: r
      when: >
        instances
        |json_query("[?region=='"+r+"'].volumes[]|[?attach_existing]")
        |count > 0

    # If we found any volumes, we store them in a form that allows
    # |match_existing_volumes to match them to volume definitions for
    # instances in the next step.

    - name: Create volume name→volume table
      set_fact:
        ec2_volumes: "{{
          ec2_volumes|combine({
            item.region ~ ':' ~ item.tags.Name: item
          })
        }}"
      with_items: >
        {{ ec2_attachable_volumes|json_query('results[?volumes][].volumes') }}

    # Preprocess instance definitions through various helpful filters to
    # set things that aren't set and check things that need checking.

    - name: Expand instance definitions based on discovered information
      set_fact:
        instances: "{{
          instances
          |expand_instance_image(ec2_region_amis)
          |expand_instance_volumes(ec2_ami_properties)
          |match_existing_volumes(cluster_name, ec2_volumes)
        }}"
      tags: ec2

    - name: Ensure that we know security group ids in every region
      assert:
        msg: "Group reference undefined"
        that:
          - ec2_region_groups[item.region] is defined
      with_items: "{{ instances }}"

    - name: Ensure that we know subnet ids in every region
      assert:
        msg: "Subnet reference undefined"
        that:
          - ec2_region_subnets[item.region] is defined
          - ec2_region_subnets[item.region][item.subnet] is defined
      with_items: "{{ instances }}"

    - name: Debug user-data for instances
      debug:
        msg: "{{
          lookup('template', 'user-data.j2',
            template_vars=dict(image=ec2_ami_properties[item.image])
          )
        }}"
      with_items: "{{ instances }}"

    # Create EC2 instances using the VPC subnets and security groups and
    # access key we configured above. We loop over instances[], look up
    # the VPC and group corresponding to the region/subnet defined for
    # that instance, and extract the relevant subnet or group id from
    # the responses registered above.

    - name: Set up EC2 instances
      ec2:
        exact_count: 1
        count_tag:
          Cluster: "{{ cluster_name }}"
          node: "{{ item.node }}"
        image: "{{ item.image }}"
        region: "{{ item.region }}"
        key_name: "{{ ec2_instance_key|default(omit) }}"
        instance_type: "{{ item.type }}"
        instance_tags: >
          {{
            cluster_tags|combine(item.tags)|combine({
              'Cluster': cluster_name,
              'node': item.node,
              'Name': item.Name,
            })
          }}
        instance_profile_name: "{{ instance_profile_name|default(omit) }}"
        private_ip: "{{ item.private_ip|default(omit) }}"
        vpc_subnet_id: "{{ ec2_region_subnets[item.region][item.subnet] }}"
        group_id: "{{ ec2_region_groups[item.region] }}"
        volumes: "{{ item.volumes|reject('has_subkey','volume_id')|list|default(omit) }}"
        spot_price: "{{ item.spot_price|default(omit) }}"
        spot_wait_timeout: "{{ item.spot_wait_timeout|default(omit) }}"
        spot_launch_group: "{{ item.spot_launch_group|default(omit) }}"
        user_data: "{{
          lookup('template', 'user-data.j2',
            template_vars=dict(image=ec2_ami_properties[item.image])
          )
        }}"
        termination_protection: "{{ item.termination_protection|default('no') }}"
        assign_public_ip: "{{ item.assign_public_ip|default('yes') }}"
        wait: yes
      with_items: "{{ instances }}"
      register: ec2_instances
      async: 7200
      poll: 0
      tags: ec2

    - name: Wait for instance provisioning to complete
      async_status: jid={{ item.ansible_job_id }}
      register: ec2_jobs
      until: ec2_jobs.finished
      retries: 300
      with_items: "{{ ec2_instances.results }}"
      tags: ec2

    # Tag all volumes attached to the instances we provisioned.

    - name: Tag volumes attached to instances
      ec2_tag:
        state: present
        region: "{{ item.1.region }}"
        resource: "{{ item.1.block_device_mapping[item.2].volume_id }}"
        tags: >
          {{
            cluster_tags|combine({
              "Name": cluster_name + ':' + item.1.tags.node + ':' + item.2,
              "CreatingCluster": cluster_name,
            })
          }}
      with_nested_dependents:
        - ec2_jobs.results
        - item.0.tagged_instances
        - item.1.block_device_mapping.keys()|list
      tags: ec2

    # If explicitly requested, we can also associate an elastic IP with
    # one or more of the instances created above. The instance ids come
    # from the results above, and the configuration is in instances[].

    - name: Associate elastic IPs with instances
      ec2_eip:
        state: present
        region: "{{ item.1.tagged_instances[0].region }}"
        device_id: "{{ item.1.tagged_instances[0].id }}"
        reuse_existing_ip_allowed: true
        in_vpc: true
      when: item.0.assign_elastic_ip|d() and item.0.assign_elastic_ip
      with_together:
        - "{{ instances }}"
        - "{{ ec2_jobs.results }}"
      register: ec2_eips
      tags: ec2_eips

    # Now we have ec2_public_ips in ec2_jobs.results, but some of them
    # may have been overriden by elastic IP addresses.

    - name: Set instance variables and elastic IP overrides
      set_fact:
        instance_vars: "{{
          instance_vars|default([])|union([
            item.0.tagged_instances[0]|combine({
              'ip_address': item.1.public_ip|default(item.0.tagged_instances[0].public_ip)
                or item.0.tagged_instances[0].private_ip,
              'public_ip': item.1.public_ip|default(item.0.tagged_instances[0].public_ip),
              'Name': item.0.item.item.Name,
              'node': item.0.item.item.node,
              'role': item.0.item.item|try_subkey('role', []),
              'backup': item.0.item.item|try_subkey('backup'),
              'upstream': item.0.item.item|try_subkey('upstream'),
              'volumes': item.0.item.item|try_subkey('volumes', []),
              'vars': item.0.item.item|try_subkey('vars', {}),
            })
          ])
        }}"
      with_together:
        - "{{ ec2_jobs.results }}"
        - "{{ ec2_eips.results }}"

    # We update A records for any instance with a route53_hosted_zone_id
    # and route53_hosted_zone (or if they are specified globally, which
    # applies to all instances).

    - name: Update DNS records in Route53
      route53:
        command: create
        zone: "{{ item.0.route53_zone|default(route53_zone) }}"
        hosted_zone_id: "{{ item.0.route53_hosted_zone_id|default(route53_hosted_zone_id) }}"
        record: "{{ item.1.Name }}.{{ item.0.route53_zone|default(route53_zone) }}"
        value: "{{ item.1.ip_address }}"
        type: A
        ttl: 60
        wait: yes
      with_together:
        - "{{ instances }}"
        - "{{ instance_vars }}"
      when:
        (item.0.route53_hosted_zone_id|d() or route53_hosted_zone_id|d()) and
        (item.0.route53_zone|d() or route53_zone|d())
      tags: route53

    # Write inventory and known_hosts for the newly-provisioned hosts.

    - include: ../common/inventory/write.yml
      vars:
        ansible_user: "{{ ec2_ami_user|default('admin') }}"
        ansible_port: "{{ cluster_ssh_port|default(22) }}"
