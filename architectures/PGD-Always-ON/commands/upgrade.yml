---

# © Copyright EnterpriseDB UK Limited 2015-2023 - All rights reserved.

- import_playbook: "{{ tpa_dir }}/architectures/lib/init.yml"
  vars:
    upgrade_in_progress: true
  tags: always

# This command performs upgrades for PGD-Always-ON clusters, including
# Postgres (minor versions only) and BDR (major: 4→5 or minor: 5.x→5.y)
# upgrades.
#
# Upgrading from BDR v4 to v5 means changing the cluster's architecture
# from BDR-Always-ON to PGD-Always-ON using `tpaexec reconfigure`, and
# then running `tpaexec upgrade`. Minor-version upgrades don't require
# any configuration changes.
#
# The overall process is similar either way:
#
# 1. Check that the cluster looks normal before doing anything.
# 2. Then, for each Postgres instance:
#    a. Fence the node (depending on the configuration)
#    b. Stop, update, and restart Postgres
#    c. Unfence the node if necessary
#    d. Install and configure new packages (again, depending on the
#       cluster's configuration)
#    e. Wait for the cluster to settle before moving on
# 3. Perform some additional tasks that are relevant only to major
#    version upgrades:
#    a. Repeat v5-specific BDR configuration, including:
#       i. Set node_kind for each instance
#       ii. Create node groups and set group options
#       iii. Create and configure proxies
#    b. Replace harp-proxy with pgd-proxy
#    c. Completely remove harp
# 4. Check that the cluster looks normal at the end
#
# Extra tasks relevant only to the BDR major version upgrade are made
# conditional on "bdr_major_version_upgrade" throughout.

- name: Check preconditions to upgrade cluster {{ cluster_dir }}
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('all') }}"
  tasks:
  - include_role: name=postgres/facts
    vars:
      pgdata_initialised: true
    when: >
      'postgres' in role and cluster_facts is not defined

  - assert:
      that:
        - cluster_facts is defined
      fail_msg: >
        Couldn't perform cluster discovery. Are all nodes running?

  - set_fact:
      bdrdb_major_version:
        "{{ cluster_facts.databases[bdr_database].bdr.bdr_major_version }}"

  - name: Ensure that bdr_version in config.yml is 5, and the database has v4/v5
    assert:
      that:
        - bdr_version is version('5', '==')
        - bdrdb_major_version is version('4', '==')
          or bdrdb_major_version is version('5', '==')
      fail_msg: >-
        This upgrade process supports upgrades from BDR v4 to PGD v5, or
        minor version upgrades of PGD v5 only

  - name: Collect list of all unique BDR versions from all nodes
    set_fact:
      all_bdr_versions: "{{ _versions.values()|list }}"
      all_bdr_major_versions:
        "{{ _versions.values()
            |map('regex_replace', '\\..*', '')
            |unique|list }}"
    vars:
      _versions:
        "{{ groups['role_bdr']|bdr_node_versions(hostvars) }}"

  - name: Determine whether we're doing a major version upgrade from BDR 4
    set_fact:
      bdr_major_version_upgrade:
        "{{ '4' in all_bdr_major_versions }}"

  # We already know that a bdr_major_version_upgrade requires 5.2, so we
  # require at least 4.3 to start with, because older BDR versions don't
  # have an extension upgrade path to 5.2.

  - name: Ensure that BDR major version upgrades start from a recent version of BDR v4
    assert:
      that: all_bdr_versions|select('version', '4.3', '<')|list is empty
      fail_msg: >-
        Please upgrade to the latest available BDR 4.x release before trying to
        upgrade to PGD 5.2
    when:
      bdr_major_version_upgrade
      and inventory_hostname == first_bdr_primary

  - name: Log exact versions for every BDR node
    debug:
      msg: >
        Running {{ bdr_major_version_upgrade|ternary('major', 'minor') }}
        version upgrade of BDR on cluster with hosts:
        {% for k in bdr_versions.keys() %}{{ k }} ({{ bdr_versions[k] }}){% if not loop.last %}, {% endif %}{% endfor %}
    vars:
      bdr_versions:
        "{{ groups['role_bdr']|bdr_node_versions(hostvars) }}"
    when:
      inventory_hostname == first_bdr_primary

  - name: Log pgd-cli diagnostic output
    include_role:
      name: test
      tasks_from: pgd-cli/info.yml
    vars:
      bdr_version: "{{ bdrdb_major_version }}"
    when:
      inventory_hostname == first_bdr_primary

- name: Set upgrade facts for all hosts
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('all') }}"
  tasks:

  # We set bdr_major_version_upgrade for all BDR instances above (and we
  # know that they must all have the same value). Now we extend that for
  # other hosts, e.g., standalone pgd-proxy instances.

  - name: Determine whether we're doing a major version upgrade from BDR 4
    set_fact:
      bdr_major_version_upgrade:
        "{{ groups['role_bdr']
            |map('extract', hostvars, 'bdr_major_version_upgrade')
            |select('defined')
            |first }}"
    when:
      bdr_major_version_upgrade is not defined

  - set_fact:
      enable_proxy_monitoring:
        "{{ enable_proxy_monitoring|default(false) }}"

# As a precaution, we refuse to start any upgrade if any proxies are
# unhealthy.
#
# Most likely, we'll encounter instances here that have both harp-proxy
# and pgd-proxy in their role (which may or may not also be data nodes),
# and we must use bdr_major_version_upgrade to decide what to do.

- name: Check proxy status before upgrade
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('role_harp-proxy:role_pgd-proxy:role_pgbouncer') }}"
  tasks:
  # XXX Should probably be moved to init:postgres.yml

  - name: Record harp-proxy endpoint DSN for monitoring
    set_fact:
      harp_proxy_endpoint:
        "{{ bdr_node_route_dsn|dbname(bdr_database, user=postgres_user, port=harp_proxy_port) }}"
    when: >
      'harp-proxy' in role

  # If a bdr_major_version_upgrade fails or is interrupted after we have
  # upgraded the data nodes to v5, but before we can replace harp-proxy
  # with pgd-proxy, a subsequent upgrade will (correctly) think it's a
  # minor version upgrade, but should still not run the pgd-proxy test.

  - set_fact:
      harp_proxy_still_running: false

  - name: Check for unreplaced harp-proxy processes
    when: >
      'harp-proxy' in role
      and 'pgd-proxy' in role
      and not bdr_major_version_upgrade
    block:
    - name: Check if harp-proxy is running
      command: pgrep harp-proxy
      register: harp_proxy_check
      ignore_errors: yes

    - set_fact:
        harp_proxy_still_running: "{{ harp_proxy_check.stdout != '' }}"

  - name: Check harp-proxy status
    include_role:
      name: test
      tasks_from: harp-proxy/basic.yml
    when: >
      'harp-proxy' in role
      and (bdr_major_version_upgrade
        or harp_proxy_still_running)

  - name: Check pgd-proxy status
    include_role:
      name: test
      tasks_from: pgd-proxy/basic.yml
    when: >
      'pgd-proxy' in role
      and not bdr_major_version_upgrade
      and not harp_proxy_still_running

  - name: Check pgbouncer configuration and status
    include_role:
      name: test
      tasks_from: pgbouncer/basic.yml
    when: >
      'pgbouncer' in role

# Before we start installing new packages on the nodes, we make any
# changes to the repository configuration that may be required. For
# example, during a BDR4→PGD5 upgrade, we have to switch to the new
# Cloudsmith repositories.

- name: Update repository configuration, if required
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('all') }}"
  tasks:
  - name: Configure local-repo, if available
    include_role:
      name: sys/local_repo

  - name: Set up repositories
    include_role:
      name: sys/repositories

  # Do the repositories we configured actually contain the packages we
  # need for the upgrade? Unfortunately, we can't always tell for sure,
  # especially with upgrades, without actually trying to install them.
  # Still, any errors we can catch now, before we stop Postgres on any
  # instances, are most helpful.
  #
  # (In principle, we ought to check that every instance has ALL the
  # packages it needs, e.g., pgd-proxy or pgbouncer. However, to avoid
  # adding a lot of potentially-confusing noise to the process, and in
  # light of the fact that there are very few repositories involved, it
  # is enough to check that the major packages, i.e., Postgres/BDR, are
  # available on one instance.)

  - name: Check if required packages are available
    include_role:
      name: postgres/pkg
      tasks_from: check-update.yml
    when: >
      'bdr' in role
      and inventory_hostname == first_bdr_primary

# When doing a BDR major version upgrade, we start a program that stays
# connected to the active proxies and repeatedly runs queries, reporting
# on any interruptions (some brief interruptions are expected).

- name: Start monitoring proxy downtime
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('role_bdr') }}"
  tasks:
  - include_role:
      name: test
      tasks_from: proxy-monitor/start.yml
    vars:
      conninfos:
        "{{ groups['role_harp-proxy']|default([])
            |map('extract', hostvars, 'harp_proxy_endpoint')
            |list }}"
    when:
      inventory_hostname == first_bdr_primary
      and enable_proxy_monitoring

# Now that we're convinced the cluster is in a reasonable initial state,
# we start the update process one by one on the instances. You can run
# the command with ``-e update_hosts=a,b,c,…`` to control the order in
# which we attempt the updates (for example, to make sure that primary
# instances are updated last in the cycle).

- name: Update postgres on instances in cluster {{ cluster_dir }}
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('role_bdr') }}"
  serial: 1
  tasks:
  - name: Fence the node before upgrade
    include_role:
      name: postgres/bdr
      tasks_from: fence.yml
    vars:
      failover_manager:
        "{{ bdr_major_version_upgrade|ternary('harp', 'pgd') }}"
    when:
      - "'bdr' in role"
      - inventory_hostname in first_bdr_primary_candidates

  - include_role:
      name: pgd_proxy/dbuser
      apply:
        tags: [pgd-proxy, dbuser]
    when:
      failover_manager == 'pgd'
      and 'bdr' in role
      and 'pem-server' not in role

  - name: Check to perform major upgrade for Postgres
    set_fact:
      postgres_major_version_upgrade: "{{ cluster_facts.postgres_version is version(postgres_version, 'lt') }}"

  - name: Stop/upgrade Postgres on {{ inventory_hostname }}
    block:
    - include_role: name=postgres/bdr tasks_from=fence.yml
      when: >
        'readonly' not in role and 'witness' not in role
    - include_role: name=postgres/update
    - include_role: name=postgres/bdr tasks_from=unfence.yml
      when: >
        'readonly' not in role and 'witness' not in role
    when: >
      'postgres' in role and postgres_major_version_upgrade

  - name: Stop/update/restart Postgres on {{ inventory_hostname }}
    block:
    - include_role: name=postgres/restart
      vars:
        postgres_service_end_state: stopped
    - include_role: name=postgres/update
    - include_role: name=postgres/user
    - include_role: name=postgres/config
    - include_role: name=postgres/final
    when: >
      'postgres' in role and not postgres_major_version_upgrade

  - set_fact:
      any_harp_proxy_still_running:
        "{{ groups['role_harp-proxy']|default([])
            |map('extract', hostvars, 'harp_proxy_still_running')
            |select('defined')
            |contains(true) }}"

  - name: Create symlink for harp-manager to connect to database
    file:
      name: "/var/run/postgresql/.s.PGSQL.{{ postgres_port }}"
      src: "/var/run/edb-pge/.s.PGSQL.{{ postgres_port }}"
      state: "link"
    when: >
      postgres_flavour == 'edbpge'
      and 'bdr' in role
      and (bdr_major_version_upgrade
        or any_harp_proxy_still_running)

  - name: Unfence the node after upgrade
    include_role:
      name: postgres/bdr
      tasks_from: unfence.yml
    vars:
      failover_manager:
        "{{ bdr_major_version_upgrade|ternary('harp', 'pgd') }}"
    when:
      - "'bdr' in role"
      - inventory_hostname in first_bdr_primary_candidates
      - not postgres_major_version_upgrade

  - name: Wait for bdr nodes to reach consensus before proceeding
    postgresql_query:
      conninfo: "{{ dsn|dbname(bdr_database) }}"
      query: >
        select s->>'leader_id' != '0' or s->>'leader' != '0' status from bdr.get_raft_status() s;
    register: bdr_raft
    until: bdr_raft.status == true
    retries: 60
    delay: 1
    become_user: "{{ postgres_user }}"
    become: yes
    when: >
      'bdr' in role and not postgres_major_version_upgrade

  # On instances that don't run Postgres, we can usually get away with a
  # much simpler upgrade process.

  - name: Stop/update/restart pgbouncer on {{ inventory_hostname }}
    include_role:
      name: pgbouncer
      tasks_from: upgrade.yml
    when: >
      'pgbouncer' in role

  - name: Stop/update/restart pgd-proxy on {{ inventory_hostname }}
    include_role:
      name: pgd_proxy
      tasks_from: upgrade.yml
    when: >
      'pgd-proxy' in role
      and not bdr_major_version_upgrade

  - name: Update pgd-cli on {{ inventory_hostname }}
    include_role:
      name: pgdcli
      tasks_from: upgrade.yml
    when: >
      'bdr' in role
      or 'pgdcli' in role
      or 'pgd-proxy' in role

  # Ansible won't stop execution if a host becomes unreachable, despite
  # the any_errors_fatal setting. We must detect that situation and fail
  # so that we don't proceed to the next host if something went wrong in
  # upgrading this one.

  - name: Fail if any host became unreachable
    assert:
      msg: "One or more hosts are no longer reachable"
      that:
        ansible_play_hosts == ansible_play_hosts_all

- name: Update BDR v5-specific configuration
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('role_bdr') }}"
  tasks:
  - include_role:
      name: postgres/bdr
      tasks_from: update.yml
    when: >
      not postgres_major_version_upgrade

  - name: Log pgd-cli diagnostic output after BDR group configuration
    include_role:
      name: test
      tasks_from: pgd-cli/info.yml
    when:
      inventory_hostname == first_bdr_primary

  - name: Wait for write leader elections to complete
    command:
      pgd show-groups -o json
    register: pgd_show_groups
    become_user: "{{ postgres_user }}"
    become: yes
    vars:
      group_info: "{{ (pgd_show_groups.stdout or '[]')|from_json }}"
      leaderless_routing_groups: >-
        {{ group_info
           |json_query("[?enable_proxy_routing && write_lead == '']") }}
      election_timeout: >-
        {{ cluster_facts.pg_settings["bdr.raft_global_election_timeout"]
           |default(6000) }}
    until:
      pgd_show_groups is not successful
      or leaderless_routing_groups is empty
    retries: "{{ (4*(election_timeout|int)/1000)|int }}"
    delay: 1
    when:
      inventory_hostname == first_bdr_primary

# During a BDR major version upgrade, all harp-proxy instances remain
# generally available throughout the process (other than the two brief
# interruptions when we fence the active node). Once the node-by-node
# process is complete, we must transfer routing responsibilities from
# harp-proxy to pgd-proxy.

- name: Update proxies in cluster {{ cluster_dir }}
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('role_harp-proxy:role_pgd-proxy:role_pgbouncer') }}"
  tasks:

  - name: Set up pgd-proxy
    include_role:
      name: pgd_proxy
    when: >
      'pgd-proxy' in role

  - name: Stop harp-proxy
    service:
      name: harp-proxy
      state: stopped
    when: >
      'harp-proxy' in role
      and (bdr_major_version_upgrade
        or harp_proxy_still_running)

  - name: Start pgd-proxy
    service:
      name: pgd-proxy
      state: started
    when: >
      'pgd-proxy' in role

  - name: Log pgd-cli diagnostic output
    block:
    - include_role:
        name: test
        tasks_from: pgd-cli/info.yml
    when: >
      'pgd-proxy' in role
    vars:
      with_user: "{{ pgd_proxy_user }}"
    run_once: true

  # No matter what sort of upgrade we're doing, pgd-proxy should be
  # correctly routing traffic now.

  - name: Check pgd-proxy status
    include_role:
      name: test
      tasks_from: pgd-proxy/basic.yml
    when: >
      'pgd-proxy' in role

- name: Stop monitoring proxy downtime
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('role_bdr') }}"
  tasks:
  - include_role:
      name: test
      tasks_from: proxy-monitor/report.yml
    when:
      inventory_hostname == first_bdr_primary
      and enable_proxy_monitoring

- name: Log diagnostic output after upgrade
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('role_bdr') }}"
  tasks:
  - name: Log pgd-cli diagnostic output
    include_role:
      name: test
      tasks_from: pgd-cli/info.yml
    when:
      inventory_hostname == first_bdr_primary

# The node-by-node upgrade process is now complete, and we have replaced
# harp-proxy with pgd-proxy for a bdr_major_version_upgrade. The cluster
# should be back to normal now, and we can take the time to get rid of
# harp completely.
#
# We do this cleanup only if the harp config file exists, and we remove
# it afterwards (so that we don't repeat these steps during subsequent
# minor-version upgrades.)
#
# (Note: we don't remove etcd, because we can't be completely sure that
# nobody else was using it.)

- name: Clean up old services in the cluster
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('all') }}"
  tasks:

  - name: Check for harp config.yml to determine if cleanup is needed
    stat:
      path: "{{ harp_directory }}/config.yml"
    register: harp_config

  # When we stop harp-manager, it will normally try to stop Postgres,
  # which we obviously don't want. The simplest of many ways to avoid
  # this is to set the cluster as unmanaged and wait for the change to
  # propagate through the DCS before stopping harp-manager.
  #
  # Normally harp-manager should still be running at this point, but we
  # double-check before doing anything, because we want to do the right
  # cleanup even after a failed upgrade is restarted.

  - name: Get rid of harp-manager
    when:
      - "'bdr' in role"
      - harp_config.stat.exists
    block:
    - name: Check if harp-manager is running
      command: pgrep harp-manager
      register: harp_manager_check
      ignore_errors: yes

    - name: Set the cluster to unmanaged before stopping harp-manager
      when:
        harp_manager_check.stdout != ''
      block:

      # In principle, we could run 'harpctl unmanage cluster' on any one
      # node, and just wait for the change to propagate everywhere else,
      # but we're taking no chances here.

      - name: Run 'harpctl unmanage cluster'
        command:
          harpctl -f /etc/harp/config.yml unmanage cluster
        become_user: "{{ postgres_user }}"
        become: yes

      - name: Wait for cluster to become unmanaged
        command:
          harpctl -f /etc/harp/config.yml get cluster -o json
        register: harpctl_get_cluster
        become_user: "{{ postgres_user }}"
        become: yes
        retries: 60
        vars:
          cluster_info: "{{ (harpctl_get_cluster.stdout or '{}')|from_json }}"
        until:
          harpctl_get_cluster is not successful
          or ('is_managed' in cluster_info and not cluster_info.is_managed)

    - name: Ensure that harp-manager is not running
      service:
        name: harp-manager
        state: stopped

    - name: Remove symlink for harp-manager to connect to database
      file:
        name: "/var/run/postgresql/.s.PGSQL.{{ postgres_port }}"
        state: "absent"
      when: >
        postgres_flavour == 'edbpge'
        and 'bdr' in role
        and (bdr_major_version_upgrade
          or any_harp_proxy_still_running)

    - name: Remove additional harp service definitions
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /etc/systemd/system/harp-manager.service.d/override.conf
        - /etc/systemd/system/harp-manager.service.d
        - /etc/systemd/system/harp-postgres.target

    - name: Disable the harp-manager service
      service:
        name: harp-manager
        enabled: false
        daemon_reload: true

    - name: Remove the harp-manager package
      package:
        name: harp-manager
        state: absent

  # There are no special considerations to removing harp-proxy, which we
  # have already stopped above. We just need to remove the package and
  # disable/remove the service.

  - name: Get rid of harp-proxy
    when: >
      'harp-proxy' in role
      and harp_config.stat.exists
    block:
    - name: Remove additional harp-proxy service definitions
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /etc/systemd/system/harp-proxy.service.d/override.conf
        - /etc/systemd/system/harp-proxy.service.d

    - name: Disable the harp-proxy service
      service:
        name: harp-proxy
        enabled: false
        daemon_reload: true

    - name: Remove the harp-proxy package
      package:
        name: harp-proxy
        state: absent

  - name: Remove harp configuration files after cleanup
    file:
      path: "{{ harp_directory }}"
      state: absent
    when:
      harp_config.stat.exists
