---

# © Copyright EnterpriseDB UK Limited 2015-2023 - All rights reserved.
#
# Upgrading from BDR v3 to v5 means changing the cluster's architecture
# from BDR-Always-ON to PGD-Always-ON using `tpaexec reconfigure`, and
# then running `tpaexec upgrade`.
#
# The overall process is similar either way:
#
# 1. Check that the cluster looks normal before doing anything.
# 2. Then, for each Postgres instance:
#    a. Fence the node (depending on the configuration)
#    b. Stop, update, and restart Postgres
#    c. Unfence the node if necessary
#    d. Install and configure new packages (again, depending on the
#       cluster's configuration)
#    e. Wait for the cluster to settle before moving on
# 3. Perform some additional tasks that are relevant only to major
#    version upgrades:
#    a. Repeat v5-specific BDR configuration, including:
#       i. Set node_kind for each instance
#       ii. Create node groups and set group options
#       iii. Create and configure proxies
#    b. Replace harp-proxy with pgd-proxy
#    c. Completely remove harp
# 4. Check that the cluster looks normal at the end
#
# Extra tasks relevant only to the BDR major version upgrade are made
# conditional on "bdr_major_version_upgrade" throughout.

- name: Check preconditions to upgrade cluster {{ cluster_dir }}
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: role_bdr
  tasks:

  # We require at least 3.7 to start with, because older BDR versions don't
  # have an extension upgrade path to 5.3.

  - name: Ensure that BDR major version upgrades start from a recent version of BDR v3
    assert:
      that: all_bdr_versions|select('version', '3.7', '<')|list is empty
      fail_msg: >-
        Please upgrade to the latest available BDR 3.x release before trying to
        upgrade to PGD 5.3
    when:
      inventory_hostname == first_bdr_primary

- name: Set upgrade facts for all hosts
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: all
  tasks:

# As a precaution, we refuse to start any upgrade if any proxies are
# unhealthy.
# TODO: Adapt the recovery used in 4to5 to 3to5. issue is that we won't get to this
# file if minor upgrade is detected. probably need to redirect per node and not once
# for the entire cluster upgrade.

- name: Check proxy status before upgrade
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: role_harp-proxy:role_pgd-proxy:role_pgbouncer
  tasks:

  - name: Record harp-proxy endpoint DSN for monitoring
    set_fact:
      harp_proxy_endpoint:
        "{{ bdr_node_route_dsn|dbname(bdr_database, user=postgres_user, port=harp_proxy_port) }}"
    when: >
      'harp-proxy' in role

  - set_fact:
      harp_proxy_still_running: false

  - name: Check for unreplaced harp-proxy processes
    when: >
      'harp-proxy' in role
      and 'pgd-proxy' in role
    block:
    - name: Check if harp-proxy is running
      command: pgrep harp-proxy
      register: harp_proxy_check
      ignore_errors: yes

    - set_fact:
        harp_proxy_still_running: "{{ harp_proxy_check.stdout != '' }}"

  - name: Check harp-proxy status
    include_role:
      name: test
      tasks_from: harp-proxy/basic.yml
    when: >
      'harp-proxy' in role
      and harp_proxy_still_running

  - name: Check pgbouncer configuration and status
    include_role:
      name: test
      tasks_from: pgbouncer/basic.yml
    when: >
      'pgbouncer' in role

# Before we start installing new packages on the nodes, we make any
# changes to the repository configuration that may be required. For
# example, during a BDR4→PGD5 upgrade, we have to switch to the new
# Cloudsmith repositories.

- name: Update repository configuration, if required
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('all') }}"
  tasks:
  - name: Configure local-repo, if available
    include_role:
      name: sys/local_repo

  - name: Set up repositories
    include_role:
      name: sys/repositories

  # Do the repositories we configured actually contain the packages we
  # need for the upgrade? Unfortunately, we can't always tell for sure,
  # especially with upgrades, without actually trying to install them.
  # Still, any errors we can catch now, before we stop Postgres on any
  # instances, are most helpful.
  #
  # (In principle, we ought to check that every instance has ALL the
  # packages it needs, e.g., pgd-proxy or pgbouncer. However, to avoid
  # adding a lot of potentially-confusing noise to the process, and in
  # light of the fact that there are very few repositories involved, it
  # is enough to check that the major packages, i.e., Postgres/BDR, are
  # available on one instance.)

  - name: Check if required packages are available
    include_role:
      name: postgres/pkg
      tasks_from: check-update.yml
    vars:
      upgrade_from: 3
    when: >
      'bdr' in role
      and inventory_hostname == first_bdr_primary

# When doing a BDR major version upgrade, we start a program that stays
# connected to the active proxies and repeatedly runs queries, reporting
# on any interruptions (some brief interruptions are expected).

- name: Start monitoring proxy downtime
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: role_bdr
  tasks:
  - include_role:
      name: test
      tasks_from: proxy-monitor/start.yml
    vars:
      conninfos:
        "{{ groups['role_harp-proxy']|default([])
            |map('extract', hostvars, 'harp_proxy_endpoint')
            |list }}"
    when:
      inventory_hostname == first_bdr_primary
      and enable_proxy_monitoring

# Now that we're convinced the cluster is in a reasonable initial state,
# we start the update process one by one on the instances. You can run
# the command with ``-e update_hosts=a,b,c,…`` to control the order in
# which we attempt the updates (for example, to make sure that primary
# instances are updated last in the cycle).

- name: Update postgres on instances in cluster {{ cluster_dir }}
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('all') }}"
  serial: 1
  tasks:
  - name: Fence the node before upgrade
    include_role:
      name: postgres/bdr
      tasks_from: fence.yml
    vars:
      failover_manager:
        harp
    when:
      - "'bdr' in role"
      - inventory_hostname in first_bdr_primary_candidates

  - include_role:
      name: pgd_proxy/dbuser
      apply:
        tags: [pgd-proxy, dbuser]
    when:
      failover_manager == 'pgd'
      and 'bdr' in role
      and 'pem-server' not in role

  - name: Stop/update/restart Postgres on {{ inventory_hostname }}
    block:
    - include_role: name=postgres/restart
      vars:
        postgres_service_end_state: stopped
    - include_role: name=postgres/update
      vars:
        upgrade_from: 3
    - include_role: name=postgres/user
    - include_role: name=postgres/config
    - include_role: name=postgres/final
    when: >
      'postgres' in role

  - set_fact:
      any_harp_proxy_still_running:
        "{{ groups['role_harp-proxy']|default([])
            |map('extract', hostvars, 'harp_proxy_still_running')
            |select('defined')
            |contains(true) }}"

  - name: Create symlink for harp-manager to connect to database
    file:
      name: "/var/run/postgresql/.s.PGSQL.{{ postgres_port }}"
      src: "/var/run/edb-pge/.s.PGSQL.{{ postgres_port }}"
      state: "link"
    when: >
      postgres_flavour == 'edbpge'
      and 'bdr' in role
      and any_harp_proxy_still_running

  - name: Unfence the node after upgrade
    include_role:
      name: postgres/bdr
      tasks_from: unfence.yml
    vars:
      failover_manager:
        harp
    when:
      - "'bdr' in role"
      - inventory_hostname in first_bdr_primary_candidates

  - name: Wait for bdr nodes to reach consensus before proceeding
    postgresql_query:
      conninfo: "{{ dsn|dbname(bdr_database) }}"
      query: >
        select s->>'leader_id' != '0' or s->>'leader' != '0' status from bdr.get_raft_status() s;
    register: bdr_raft
    until: bdr_raft.status == true
    retries: 60
    delay: 1
    become_user: "{{ postgres_user }}"
    become: yes
    when: >
      'bdr' in role

  # On instances that don't run Postgres, we can usually get away with a
  # much simpler upgrade process.

  - name: Stop/update/restart pgbouncer on {{ inventory_hostname }}
    include_role:
      name: pgbouncer
      tasks_from: upgrade.yml
    when: >
      'pgbouncer' in role

  # We need to install pgdcli since it's not there in bdr3
  # We also need to create pgdproxy system user here for pgd-proxy nodes
  # since it will be the user for pgdcli on these nodes.
  - name: Create pgdproxy system user for pgdcli
    include_role:
      name: pgd_proxy/user
    when: >
      'pgd-proxy' in role

  - name: Install pgd-cli on {{ inventory_hostname }}
    include_role:
      name: pgdcli
    when: >
      'bdr' in role
      or 'pgdcli' in role
      or 'pgd-proxy' in role

  # Ansible won't stop execution if a host becomes unreachable, despite
  # the any_errors_fatal setting. We must detect that situation and fail
  # so that we don't proceed to the next host if something went wrong in
  # upgrading this one.

  - name: Fail if any host became unreachable
    assert:
      msg: "One or more hosts are no longer reachable"
      that:
        ansible_play_hosts == ansible_play_hosts_all

- name: Update BDR v5-specific configuration
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: role_bdr
  tasks:
  - include_role:
      name: postgres/bdr
      tasks_from: update.yml

  - name: Log pgd-cli diagnostic output after BDR group configuration
    include_role:
      name: test
      tasks_from: pgd-cli/info.yml
    when:
      inventory_hostname == first_bdr_primary

  - name: Wait for write leader elections to complete
    command:
      pgd show-groups -o json
    register: pgd_show_groups
    become_user: "{{ postgres_user }}"
    become: yes
    vars:
      group_info: "{{ (pgd_show_groups.stdout or '[]')|from_json }}"
      leaderless_routing_groups: >-
        {{ group_info
           |json_query("[?enable_proxy_routing && write_lead == '']") }}
      election_timeout: >-
        {{ cluster_facts.pg_settings["bdr.raft_global_election_timeout"]
           |default(6000) }}
    until:
      pgd_show_groups is not successful
      or leaderless_routing_groups is empty
    retries: "{{ (4*(election_timeout|int)/1000)|int }}"
    delay: 1
    when:
      inventory_hostname == first_bdr_primary

# During a BDR major version upgrade, all harp-proxy instances remain
# generally available throughout the process (other than the two brief
# interruptions when we fence the active node). Once the node-by-node
# process is complete, we must transfer routing responsibilities from
# harp-proxy to pgd-proxy.

- name: Update proxies in cluster {{ cluster_dir }}
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: role_harp-proxy:role_pgd-proxy:role_pgbouncer
  tasks:

  - name: Set up pgd-proxy
    include_role:
      name: pgd_proxy
    when: >
      'pgd-proxy' in role

  - name: Stop harp-proxy
    service:
      name: harp-proxy
      state: stopped
    when: >
      'harp-proxy' in role
      and harp_proxy_still_running

  - name: Start pgd-proxy
    service:
      name: pgd-proxy
      state: started
    when: >
      'pgd-proxy' in role

  - name: Log pgd-cli diagnostic output
    block:
    - include_role:
        name: test
        tasks_from: pgd-cli/info.yml
    when: >
      'pgd-proxy' in role
    vars:
      with_user: "{{ pgd_proxy_user }}"
    run_once: true

  # No matter what sort of upgrade we're doing, pgd-proxy should be
  # correctly routing traffic now.

  - name: Check pgd-proxy status
    include_role:
      name: test
      tasks_from: pgd-proxy/basic.yml
    when: >
      'pgd-proxy' in role

- name: Stop monitoring proxy downtime
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: role_bdr
  tasks:
  - include_role:
      name: test
      tasks_from: proxy-monitor/report.yml
    when:
      inventory_hostname == first_bdr_primary
      and enable_proxy_monitoring

- name: Log diagnostic output after upgrade
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: role_bdr
  tasks:
  - name: Log pgd-cli diagnostic output
    include_role:
      name: test
      tasks_from: pgd-cli/info.yml
    when:
      inventory_hostname == first_bdr_primary

# The node-by-node upgrade process is now complete, and we have replaced
# harp-proxy with pgd-proxy for a bdr_major_version_upgrade. The cluster
# should be back to normal now, and we can take the time to get rid of
# harp completely.
#
# We do this cleanup only if the harp config file exists, and we remove
# it afterwards (so that we don't repeat these steps during subsequent
# minor-version upgrades.)
#
# (Note: we don't remove etcd, because we can't be completely sure that
# nobody else was using it.)

- name: Clean up old services in the cluster
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: all
  tasks:

  - name: Check for harp config.yml to determine if cleanup is needed
    stat:
      path: "{{ harp_directory }}/config.yml"
    register: harp_config

  # When we stop harp-manager, it will normally try to stop Postgres,
  # which we obviously don't want. The simplest of many ways to avoid
  # this is to set the cluster as unmanaged and wait for the change to
  # propagate through the DCS before stopping harp-manager.
  #
  # Normally harp-manager should still be running at this point, but we
  # double-check before doing anything, because we want to do the right
  # cleanup even after a failed upgrade is restarted.

  - name: Get rid of harp-manager
    when:
      - "'bdr' in role"
      - harp_config.stat.exists
    block:
    - name: Check if harp-manager is running
      command: pgrep harp-manager
      register: harp_manager_check
      ignore_errors: yes

    - name: Set the cluster to unmanaged before stopping harp-manager
      when:
        harp_manager_check.stdout != ''
      block:

      # In principle, we could run 'harpctl unmanage cluster' on any one
      # node, and just wait for the change to propagate everywhere else,
      # but we're taking no chances here.

      - name: Run 'harpctl unmanage cluster'
        command:
          harpctl -f /etc/harp/config.yml unmanage cluster
        become_user: "{{ postgres_user }}"
        become: yes

      - name: Wait for cluster to become unmanaged
        command:
          harpctl -f /etc/harp/config.yml get cluster -o json
        register: harpctl_get_cluster
        become_user: "{{ postgres_user }}"
        become: yes
        retries: 60
        vars:
          cluster_info: "{{ (harpctl_get_cluster.stdout or '{}')|from_json }}"
        until:
          harpctl_get_cluster is not successful
          or ('is_managed' in cluster_info and not cluster_info.is_managed)

    - name: Ensure that harp-manager is not running
      service:
        name: harp-manager
        state: stopped

    - name: Remove symlink for harp-manager to connect to database
      file:
        name: "/var/run/postgresql/.s.PGSQL.{{ postgres_port }}"
        state: "absent"
      when: >
        postgres_flavour == 'edbpge'
        and 'bdr' in role
        and any_harp_proxy_still_running

    - name: Remove additional harp service definitions
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /etc/systemd/system/harp-manager.service.d/override.conf
        - /etc/systemd/system/harp-manager.service.d
        - /etc/systemd/system/harp-postgres.target

    - name: Disable the harp-manager service
      service:
        name: harp-manager
        enabled: false
        daemon_reload: true

    - name: Remove the harp-manager package
      package:
        name: harp-manager
        state: absent

  # There are no special considerations to removing harp-proxy, which we
  # have already stopped above. We just need to remove the package and
  # disable/remove the service.

  - name: Get rid of harp-proxy
    when: >
      'harp-proxy' in role
      and harp_config.stat.exists
    block:
    - name: Remove additional harp-proxy service definitions
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /etc/systemd/system/harp-proxy.service.d/override.conf
        - /etc/systemd/system/harp-proxy.service.d

    - name: Disable the harp-proxy service
      service:
        name: harp-proxy
        enabled: false
        daemon_reload: true

    - name: Remove the harp-proxy package
      package:
        name: harp-proxy
        state: absent

  - name: Remove harp configuration files after cleanup
    file:
      path: "{{ harp_directory }}"
      state: absent
    when:
      harp_config.stat.exists
