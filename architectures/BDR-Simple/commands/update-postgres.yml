---

# Copyright © 2ndQuadrant Limited <info@2ndquadrant.com>

- import_playbook: "{{ tpa_dir }}/architectures/lib/init.yml"
  tags: always

# We must perform a minor-version update of Postgres and BDR on a
# running cluster while maintaining overall cluster availability.
#
# The cluster may comprise BDR primary, logical standby, and physical
# streaming replica instances; and pgbouncer+haproxy instances besides.
# Logical standby and physical replica instances can be updated without
# any special precautions. Just stop, update, start.
#
# For BDR primaries, we must first check if haproxy is routing traffic
# to them, and if so, redirect to another primary before stopping and
# upgrading this one.
#
# (We don't try to minimise the downtime on each instance by downloading
# the packages before stopping the server, but we should explore that in
# future as an optimisation. Although the architecture tolerates delays,
# we would prefer the downtime to be predictable rather than dependent
# on the whims of the network.)

- name: Collect update facts for cluster {{ cluster_dir }}
  any_errors_fatal: True
  max_fail_percentage: 0
  become_user: root
  become: yes
  hosts: all
  vars:
    all_haproxy_backends: []
  tasks:

  # From the haproxy_backends settings on haproxy instances, we assemble
  # the following:
  #
  # 1. A list of all haproxy backends (all_haproxy_backends)
  # 2. A map associating each backend instance with its corresponding
  #    haproxy instance (proxy_for_backend)
  #
  # We assume for now that the haproxy instance also runs a pgbouncer
  # pointing to the haproxy (which is true of BDR-Always-ON by default).

  - name: Record haproxy/backend relationships
    set_fact:
      all_haproxy_backends: "{{
        all_haproxy_backends|default([])|union([item.1])
      }}"
      proxy_for_backend: "{{
        proxy_for_backend|default({})|combine({ item.1: item.0 })
      }}"
    with_nested_dependents:
    - groups['role_haproxy']|default([])
    - hostvars[item.0].haproxy_backends

  # As a precaution, we start the update only if the servers are healthy
  # and match the expected configuration. It's convenient to do this by
  # just rerunning the basic pgbouncer and haproxy tests.
  #
  # This does mean that if the update leaves a backend server in "maint"
  # state because a package installation fails, it will have to be fixed
  # by hand before retrying the update with a fixed configuration.

  - name: Check pgbouncer configuration and status
    include_role:
      name: test
      tasks_from: pgbouncer/basic.yml
    when: >
      'pgbouncer' in role

  - name: Check haproxy configuration and status
    include_role:
      name: test
      tasks_from: haproxy/basic.yml
    when: >
      'haproxy' in role

# Now that we're convinced the cluster is in a reasonable initial state,
# we start the update process one by one on the instances. You can run
# the command with ``-e update_hosts=a,b,c,…`` to control the order in
# which we attempt the updates (for example, to make sure that primary
# instances are updated last in the cycle).

- name: Update postgres on instances in cluster {{ cluster_dir }}
  any_errors_fatal: True
  max_fail_percentage: 0
  become_user: root
  become: yes
  hosts: "{{ update_hosts|default('all') }}"
  serial: 1
  vars:
    haproxy_stats_socket: /var/lib/haproxy/stats
    all_haproxy_backends: []
    proxy_for_backend: {}
  tasks:

  # First, we check if the current instance is named in haproxy_backends
  # for one of the haproxy instances. Thereafter, if is_proxied is true,
  # my_proxy is the name of the relevant haproxy instance.

  - set_fact:
      is_proxied: >-
        {{ inventory_hostname in all_haproxy_backends }}
      my_proxy: >-
        {{ proxy_for_backend[inventory_hostname]|default('none') }}

  # We want to set this server into the MAINT state in haproxy, which
  # disables health checks and prevents new connections to the server
  # (it does not terminate existing connections, as the documentation
  # indirectly suggests). This is equivalent to "disable server".
  #
  # We would also like to know if the server was active, so that we can
  # take some additional steps below. Unfortunately, there is no way to
  # do this in a single step.
  #
  # We can tell if haproxy is directing traffic to a server by looking
  # for an entry with server_id=NNN in the stick table for the backend
  # ("show table be"). To make sense of the server_ids, we need to look
  # at the server state ("show servers state be"). If we then switch to
  # MAINT, we can parse the output to figure out if the server had been
  # receiving traffic before the switch.
  #
  # We assume that pgbouncer is connecting to haproxy on the same host,
  # so we send "show table be key 127.0.0.1" to find only the relevant
  # entry in the stick table.
  #
  # This is not resilient against changes to the traffic routing because
  # of failed health checks during the process. The best we can do is to
  # minimise the window of opportunity for problems by issuing all three
  # commands in one go.
  #
  # Note: I have not been able to find any advantage to using the DRAIN
  # state in haproxy, or even understand exactly what it does. However,
  # it does not remove the stick table entry for the server, so we have
  # to issue an additional "clear table be key 127.0.0.1".

  - name: Put backend {{ inventory_hostname }} in MAINT mode
    shell: >
      echo "show table be key 127.0.0.1;
      show servers state be;
      set server be/{{ inventory_hostname }} state maint" |
      socat stdio {{ haproxy_stats_socket }}
    args:
      executable: /bin/bash
    register: servers
    delegate_to: "{{ my_proxy }}"
    when: is_proxied

  - set_fact:
      status_lines: "{{ servers.stdout_lines|default([]) }}"

  # We must now extract some meaning from the output of the commands we
  # sent above. The "show table …" output will be an empty line (if the
  # stick table has no entry for pgbouncer) or will look something like
  # the following:
  #
  # # table: be, type: ip, size:1, used:1
  # 0xNNNNNNNNNNNN: key=127.0.0.1 use=0 exp=0 server_id=1
  #
  # For details, see src/stick_table.c:table_dump_head_to_buffer and
  # table_dump_entry_to_buffer.
  #
  # We extract the server_id, if available, and remove any parsed output
  # from status_lines.

  - name: Set former_server_id based on stick table contents
    set_fact:
      former_active_id: "{{
        _fields[-1]|regex_replace('server_id=', '')
      }}"
      status_lines: "{{ status_lines[2:] }}"
    vars:
      _fields: "{{ status_lines[1].split(' ') }}"
    when: >
      status_lines and
      status_lines[0].startswith("# table: be,")

  - set_fact:
      status_lines: "{{ status_lines[1:] }}"

  # Next up, the "show servers …" output will look like this:
  #
  # 1
  # # be_id be_name srv_id srv_name srv_addr srv_op_state srv_admin_state …
  # 3 be 1 quart 10.33.244.222 2 0 1 1 5376 15 3 7 6 0 0 0 quart 5432 -
  # 3 be 2 zephyr 10.33.244.215 2 0 1 1 5376 15 3 7 6 0 0 0 zephyr 5432 -
  #
  # See doc/management.txt in the haproxy source for details about the
  # meaning of these values.
  #
  # If we found a former_active_id above, we translate that to a server
  # name here. (Otherwise we know there's no active server yet, so there
  # is no need to treat this instance specially.)

  - name: Set former_server_name based on server state
    set_fact:
      former_active_server: "{{ _fields[3] }}"
    with_items:
      "{{ status_lines[2:] }}"
    vars:
      _fields: "{{ item.split(' ') }}"
    when: >
      status_lines and
      status_lines[0] == "1" and
      status_lines[1].startswith("# be_id be_name ") and
      former_active_id is defined and
      former_active_id == _fields[2]

  - set_fact:
      was_active: "{{
        inventory_hostname == former_active_server|default('none')
      }}"
    when: is_proxied

  # If this was an active backend, we must ask pgbouncer to migrate
  # connections away from it and wait for that process to complete
  # before we shut it down.

  - name: Issue RECONNECT and WAIT_CLOSE to pgbouncer
    postgresql_query:
      conninfo: "{{ _dsn|dbname('pgbouncer', user='pgbouncer') }}"
      autocommit: yes
      queries:
      - text: RECONNECT
      - text: WAIT_CLOSE
    vars:
      _dsn: "{{ hostvars[my_proxy].pgbouncer_node_dsn }}"
    delegate_to: "{{ my_proxy }}"
    environment:
      PGPASSWORD: "{{ pgbouncer_password }}"
    when:
      is_proxied and was_active

  # Once the server is guaranteed to have no active traffic, we can wait
  # to ensure that any writes it originated have finished replicating to
  # the other backends. This is just a convenience, because the instance
  # would replicate after the restart anyway.

  - name: Wait for replication to complete
    postgresql_query:
      conninfo: "{{ dsn|dbname(bdr_database) }}"
      query: >
        select bdr.wait_slot_confirm_lsn(NULL, NULL)
    become_user: "{{ postgres_user }}"
    become: yes
    when:
      is_proxied

  # Now we can perform the update on Postgres instances, whether proxied
  # (e.g., primary instances) or not (e.g., logical standby instances).
  # We don't do anything for non-Postgres instances at the moment.

  - name: Stop/update/restart Postgres on {{ inventory_hostname }}
    block:
    - include_role: name=postgres/restart
      vars:
        postgres_service_end_state: stopped
    - include_role: name=postgres/update
    - include_role: name=postgres/restart
      vars:
        postgres_service_end_state: started
    when: >
      'postgres' in role

  # After we update one instance in a CAMO pair, we must wait for its
  # partner to catch up. If we stop the partner before it is ready, we
  # will lose the status of transactions committed on the origin.

  - name: Wait for CAMO partner to be ready before proceeding
    postgresql_query:
      conninfo: "{{ dsn|dbname(bdr_database) }}"
      query: >
        SELECT bdr.is_camo_partner_connected() AND
          bdr.is_camo_partner_ready() AS ready
    register: camo_status
    until: camo_status.ready
    retries: 180
    become_user: "{{ postgres_user }}"
    become: yes
    when: bdr_node_camo_partner is defined

  # Once that's done, we can inform haproxy that this backend is once
  # again ready to accept connections.

  - name: Declare backend {{ inventory_hostname }} as ready
    shell: >
      echo "set server be/{{ inventory_hostname }} state ready" |
      socat stdio {{ haproxy_stats_socket }}
    args:
      executable: /bin/bash
    delegate_to: "{{ my_proxy }}"
    when: is_proxied

  # Ansible won't stop execution if a host becomes unreachable, despite
  # the any_errors_fatal setting. We must detect that situation and fail
  # so that we don't proceed to the next host if something went wrong in
  # upgrading this one.

  - name: Fail if any host became unreachable
    assert:
      msg: "One or more hosts are no longer reachable"
      that:
        ansible_play_hosts == ansible_play_hosts_all
