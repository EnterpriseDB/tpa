---

# © Copyright EnterpriseDB UK Limited 2015-2025 - All rights reserved.

- import_playbook: "{{ tpa_dir }}/architectures/lib/init.yml"
  vars:
    upgrade_in_progress: true
  tags: always

# We must perform a minor-version update of Postgres and BDR on a
# running cluster while maintaining overall cluster availability.
#
# The cluster may comprise BDR primary, logical standby, and physical
# streaming replica instances; and pgbouncer+haproxy instances besides.
# Logical standby and physical replica instances can be updated without
# any special precautions. Just stop, update, start.
#
# For BDR primaries, we must first check if haproxy is routing traffic
# to them, and if so, redirect to another primary before stopping and
# upgrading this one.
#
# (We don't try to minimise the downtime on each instance by downloading
# the packages before stopping the server, but we should explore that in
# future as an optimisation. Although the architecture tolerates delays,
# we would prefer the downtime to be predictable rather than dependent
# on the whims of the network.)

- name: Check that the cluster can be upgraded
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: role_bdr
  tasks:
  - assert:
      that:
        - cluster_facts is defined
      fail_msg: >
        Couldn't perform cluster discovery. Are all nodes running?

  - set_fact:
      bdrdb_major_version:
        "{{ cluster_facts.databases[bdr_database].bdr.bdr_major_version }}"


- name: Collect update facts for cluster {{ cluster_dir }}
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: all
  tasks:

  # Set the bdrdb_major_version on non bdr node
  # used for pgd-cli to ensure we install configuration file
  # if upgrading from bdr3 to bdr4.
  - set_fact:
      bdrdb_major_version: "{{ groups['role_bdr']
            |map('extract', hostvars, 'bdrdb_major_version')
            |select('defined')
            |first }}"
    when: >
      'bdr' not in role

  # Set all_haproxy_backend_servers and proxies_for_backend_server, as
  # well as my_haproxies.

  - include_role:
      name: haproxy
      tasks_from: map-backend-proxies.yml

  # As a precaution, we start the update only if the servers are healthy
  # and match the expected configuration. It's convenient to do this by
  # just rerunning the basic pgbouncer and haproxy tests.
  #
  # This does mean that if the update leaves a backend server in "maint"
  # state because a package installation fails, it will have to be fixed
  # by hand before retrying the update with a fixed configuration.

  - name: Check pgbouncer configuration and status
    include_role:
      name: test
      tasks_from: pgbouncer/basic.yml
    when: >
      'pgbouncer' in role

  - name: Check haproxy configuration and status
    include_role:
      name: test
      tasks_from: haproxy/basic.yml
    when: >
      'haproxy' in role

# Now that we're convinced the cluster is in a reasonable initial state,
# we start the update process one by one on the instances. You can run
# the command with ``-e update_hosts=a,b,c,…`` to control the order in
# which we attempt the updates (for example, to make sure that primary
# instances are updated last in the cycle).

- name: Update repository configuration, if required
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('all') }}"
  tasks:
  - name: Configure local-repo, if available
    include_role:
      name: sys/local_repo

  - name: Set up repositories
    include_role:
      name: sys/repositories

- name: Update postgres on instances in cluster {{ cluster_dir }}
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "{{ update_hosts|default('all') }}"
  serial: 1
  tasks:

  # We mark this backend as disabled in haproxy and tell pgbouncer to
  # reconnect to haproxy and wait for active connections to be closed.

  - include_role:
      name: haproxy
      tasks_from: disable-backend.yml
    when: >
      'role_haproxy' in groups

  - name: Fence the node before upgrade
    include_role:
      name: postgres/bdr
      tasks_from: fence.yml
    when:
      - "'bdr' in role"
      - failover_manager == 'harp'
      - inventory_hostname in first_bdr_primary_candidates

  # We have to stop harp-manager to upgrade it, and it might stop and
  # restart Postgres when we do, so we do it after fencing the node.

  - name: Stop/update/restart harp on {{ inventory_hostname }}
    include_role:
      name: harp
      tasks_from: upgrade.yml
    when: >
      'harp-proxy' in role
      or ('bdr' in role
        and failover_manager == 'harp'
        and inventory_hostname in first_bdr_primary_candidates|default([]))

  # Now we can perform the update on Postgres instances, whether proxied
  # (e.g., primary instances) or not (e.g., logical standby instances).
  #
  # We filter out 'replica' instances when running postgres/config/final
  # since the idea around postgres/config/final is to do make changes
  # that involve a running postgres instance, e.g. tasks like creating
  # databses, extensions, users etc. This is unlike postgres/config that
  # should render similar settings on both primary and replica so they
  # match. Is there a scenario where a replica may have some setting
  # that is different from the primary and could be done on a running
  # postgres instance? Maybe, but we don't recommend replicas drifting
  # in terms of configuration purely because a replica is supposed to
  # replace a primary in case of a failover.

  - name: Stop/update/restart Postgres on {{ inventory_hostname }}
    block:
    - include_role: name=postgres/restart
      vars:
        postgres_service_end_state: stopped
    - include_role: name=postgres/update
      vars:
        upgrade_from: "{{ bdrdb_major_version }}"
    - include_role: name=postgres/config
    - include_role: name=postgres/restart
    - include_role: name=postgres/config/final
      vars:
        postgres_service_end_state: started
      when: >
        'replica' not in role
    when: >
      'postgres' in role
      and 'pem-server' not in role

  - name: Upgrade postgres instance on the pem server
    block:
    - include_role: name=postgres/restart
    - include_role: name=postgres/update
    - include_role: name=postgres/config
    - include_role: name=postgres/restart
    - include_role: name=postgres/config/final
      vars:
        postgres_service_end_state: started
    when: >
      'pem-server' in role

  - name: Stop/update/restart pgbouncer on {{ inventory_hostname }}
    include_role:
      name: pgbouncer
      tasks_from: upgrade.yml
    when: >
      'pgbouncer' in role

  - name: Stop/update/restart etcd on {{ inventory_hostname }}
    include_role:
      name: etcd
      tasks_from: upgrade.yml
    when: >
      'etcd' in role

  - name: Upgrade pgd-cli on {{ inventory_hostname }}
    block:
      - name: Upgrade pgdcli package
        include_role:
          name: pgdcli
          tasks_from: upgrade.yml

      - name: Configure pgdcli when upgrading from bdr3
        include_role:
          name: pgdcli/config
        when:
        - bdrdb_major_version|int == 3
        - bdr_version is version('4', '>=')
    when: >
      ('bdr' in role and bdr_version is version('4', '>='))
      or 'pgdcli' in role

  # After we update one instance in a CAMO pair, we must wait for its
  # partner to catch up. If we stop the partner before it is ready, we
  # will lose the status of transactions committed on the origin.

  - name: Wait for CAMO partner to be ready before proceeding
    postgresql_query:
      conninfo: "{{ dsn|dbname(bdr_database) }}"
      query: >
        SELECT bdr.is_camo_partner_connected() AND
          bdr.is_camo_partner_ready() AS ready
    register: camo_status
    until: camo_status.ready
    retries: 180
    become_user: "{{ postgres_user }}"
    become: yes
    when: bdr_node_camo_partner is defined

  # Once that's done, we can inform haproxy that this backend is once
  # again ready to accept connections.

  - name: Declare backend {{ inventory_hostname }} as ready
    shell: >
      echo "set server be/{{ inventory_hostname }} state ready" |
      socat stdio "{{ haproxy_stats_socket }}"
    args:
      executable: /bin/bash
    with_items: "{{ my_haproxies }}"
    delegate_to: "{{ item }}"

  - name: Unfence the node after upgrade
    include_role:
      name: postgres/bdr
      tasks_from: unfence.yml
    when:
      - "'bdr' in role"
      - failover_manager == 'harp'
      - inventory_hostname in first_bdr_primary_candidates

  # Ansible won't stop execution if a host becomes unreachable, despite
  # the any_errors_fatal setting. We must detect that situation and fail
  # so that we don't proceed to the next host if something went wrong in
  # upgrading this one.

  - name: Fail if any host became unreachable
    assert:
      msg: "One or more hosts are no longer reachable"
      that:
        ansible_play_hosts == ansible_play_hosts_all

# BDR 4.3 adds node_kind to fix some issues and it is advised to set it
# when moving to 4.3+ we need to wait after all nodes are updated to set
# it for all bdr nodes.

- name: Set node_kind
  any_errors_fatal: true
  max_fail_percentage: 0
  become_user: root
  become: yes
  environment: "{{ target_environment }}"
  hosts: "role_bdr"
  tasks:

  - block:

    # we expect all node to haave gone through upgrade process so we
    # only check that we effectively upgraded to 4.3+ on first_bdr_primary
    # before trying to make the change.
    - name: Get bdr_version_num
      postgresql_query:
        conninfo: "{{ bdr_node_dsn }}"
        queries:
          - text: >
              SELECT bdr.bdr_version_num()
      become_user: "{{ postgres_user }}"
      become: yes
      register: query_version

    - name: Ensure that all BDR nodes have an appropriate node_kind set
      postgresql_query:
        conninfo: "{{ bdr_node_dsn }}"
        queries:
          - text: >
              SELECT bdr.alter_node_kind(node_name := %s, node_kind := %s)
                FROM bdr.node where node_name = %s and node_kind = 0;
            args:
              - "{{ this_bdr_node_name }}"
              - "{{ bdr_node_role|bdr_node_kind }}"
              - "{{ this_bdr_node_name }}"
      become_user: "{{ postgres_user }}"
      become: yes
      with_items: "{{ groups['role_bdr'] }}"
      loop_control:
        loop_var: bdr_node
        label: >-
          {{ bdr_node }}:{{ bdr_node_role|bdr_node_kind }}
      vars:
        this_bdr_node_name: "{{ hostvars[bdr_node].bdr_node_name }}"
        bdr_node_role: "{{ hostvars[bdr_node].role }}"
      when: >
        query_version.bdr_version_num >= 40300

    when: >
      inventory_hostname == first_bdr_primary
