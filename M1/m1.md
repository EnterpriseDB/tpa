---
title: "Trusted PostgreSQL Architecture: M1"
version: 2.0
customer: 2ndQuadrant
copyright-years: 2018-19
date: 13 February 2019
author: 2ndQuadrant
table-row-colors: false
toc: true
classification: partner-confidential
display-author-comments: false
display-revision-history: false
---

\BeginRevisions

\Revision 1.0; 2018-05-17; BM, GC: First version.

\Revision 1.1; 2018-12-17; GC: Mentioning alternate approaches on
PgBouncer node failure.

\Revision 1.2; 2018-12-28; GC: Introducing the "Two Barman, Local
Source" WIP scenario.

\Revision 1.3; 2018-12-28; GC: Updated version numbers of various
software.

\Revision 1.4; 2019-01-18; GC: Replaced WIP section with a comment, so
the file can be shared with customers.

\Revision 1.5; 2019-01-18; GC: Fixed some inaccuracies in graphs (all
repmgrs interact with all pgbouncers, irrespective of location).

\Revision 2.0 (DRAFT 2); 2019-02-02; GC: Introducing a second Barman
node; many failure scenarios reworked as appropriate.

\Revision 2.0 (DRAFT 3); 2019-02-11; GC: Using the newly available
Geo-redundancy for the second Barman node.

\Revision 2.0; 2019-02-13; AMS, GC: Publishing after Abhijit's review.

\EndRevisions

# Introduction

In this document we describe a High Availability architecture based on
PostgreSQL.

## Database Architecture

![Database Architecture Diagram\label{ha}](cache/ha.png){ width=85% }

The architecture is displayed in Figure \ref{ha}, and presents the
following characteristics:

- It is deployed over two datacenters
- Each datacenter is an *availability zone*
- The following components are included:
    - Four PostgreSQL nodes (nodes 1 - 4)
        - One node designated as Primary
        - Three nodes designated as Standby
    - Two PgBouncer nodes
    - Two Barman nodes
        - The node in the same datacenter as the Primary PostgreSQL
          node is configured as Primary
        - The one in the other datacenter is configured as Passive
    - Each PostgreSQL node runs repmgr
- The following software is used:
    - PostgreSQL 11
    - PgBouncer 1.9
    - repmgr 4.2
    - Barman 2.6
- Applications access the database layer through PgBouncer
- PostgreSQL Standby nodes 2, 3, 4 are cloned from PostgreSQL Node 1
  using repmgr, and can replace Node 1 on failure
- The Primary Barman node takes backups from the Primary PostgreSQL
  node
- WAL is transmitted using *Physical Streaming Replication* (PSR):
    - from the Primary PostgreSQL node to Standby PostgreSQL nodes 2, 3
	- from the Primary PostgreSQL node to the Primary Barman node
    - from Node 3 to Node 4, both located on the same datacenter
- The Primary Barman node uses a *replication slot* to ensure that the
  upstream node does not remove unsent WAL
- The Passive Barman node fetches WAL and backups from the Primary
  Barman node using Geo-redundancy mode
- Standby PostgreSQL nodes do not use replication slots; if they fall
  behind due to WAL streaming being disrupted, they can restore any
  missing WAL directly from the local Barman node

\AuthorComment[GC]{

TPAexec is currently forcing version 4.0.6 of repmgr because of an
incompatibility with repmgr 4.1+; the corresponding Redmine issue is
https://redmine.2ndquadrant.com/issues/4931

}

## Backups with Barman

The database is backed up using the Primary Barman node, which is
configured to perform base backups and archive WAL files from Node 1
in Datacenter A, which is initially the Primary node.

The base backup, together with the WAL archive, can be used as
follows:

- To restore a PostgreSQL node using Point-In-Time-Recovery
- To provide Standby nodes with any older WAL files that have already
  been eliminated from their upstream nodes.

\AuthorComment{

We choose WAL streaming because it provides a lower RPO.

It is also safer than the alternative of using log shipping through
rsync, although nowadays Barman provides an utility to implement log
shipping safely.

(The related failure scenario, as pointed out by √Ålvaro Herrera, is
the Barman server crashing after the WAL file is transferred by rsync,
but before it is flushed to disk. In this case the WAL file could be
missing from Barman's catalog, despite being successfully archived.)

Note also that log shipping allows parallel archiving, but we don't
consider it yet.

}

The Passive Barman nodes does not connect to PostgreSQL; it fetches
data from the Primary Barman node, to provide backup redundancy
without additional I/O pressure on the database servers.

# High Availability Procedures

In this section we describe High Availability procedures in more
detail, including failure scenarios as well as maintenance
activities. We cover the following procedures:

- [Switchover]
- [Node Upgrade]
- [Primary Node Failure]
- [Standby Node Failure]
- [Barman Node Failure]
- [PgBouncer Node Failure]
- [Datacenter Failure]

## Switchover

Note that the *Switchover* procedure is a planned intervention, as
opposed to the *Failover* procedure described in
[Primary Node Failure], designed to react to an unplanned failure.

The switchover procedure from Primary (Node 1) to a standby in the
same datacenter (Node 2) is as follows:

1. Perform a `CHECKPOINT` on the Primary Node 1
2. Pause PgBouncer
3. Pause all repmgr daemons by running `repmgr daemon pause` command on one node (e.g. Node 2)

At this point the database service is interrupted.

4. Shut down Node 1
5. Wait until Node 2 has replayed all the WAL received from Node 1
6. Promote Node 2 to Primary
7. Reconfigure PgBouncer to point to Node 2 (the new Primary)
8. "unpause" all repmgr daemons by running `repmgr daemon unpause` command on one of the nodes (e.g. Node 2)
9. Resume PgBouncer

At this point the service is restored.

10. Reconfigure Node 1 as a Standby node using the `repmgr node rejoin`
    syntax
11. Reconfigure Standby Node 3 to follow the new Primary node using
    `repmgr standby follow`
12. Reconfigure the Primary Barman node to point to Node 2, the new
    PostgreSQL Primary node

\AuthorComment{

We only have one kind of database connection defined in PgBouncer.

However, we can improve read-only availability by setting up a second
PgBouncer database and pointing it to one of the standbys.

Note that this assumes that the application is able to separate reads
from writes.

}

### Using `repmgr standby switchover`

If appropriately configured, `repmgr` allows a simplified switchover
procedure with its dedicated syntax, as follows:

i. Perform a `CHECKPOINT` on the Primary Node 1
ii. Pause PgBouncer
iii. Execute `repmgr standby switchover --siblings-follow`

The simplified switchover procedure is related to the former switchover procedure as follows:

- Step "i" is the same as Step 1
- Step "ii" is the same as Step 2
- Step "iii" includes steps 1, 3, 4, 5, 6, 8, 10
- Step "iii" also includes step 11, thanks to the `--siblings-follow` option
- Steps 7, 9 can be implemented as part of a script reacting to the
  `standby_promote` event notification
- Step 12 can be implemented as part of a script reacting to the
  `standby_switchover` event notification

In particular, the resulting sequence of events is:

- Step 1 -- explicitely executed
- Step 2 -- explicitely executed
- Step 1 again -- executed by `standby switchover`
- Steps 3, 4, 5, 6 -- executed by `standby switchover`
- Steps 7, 9 -- executed by the script reacting to the `standby_promote` event notification
- Step 8, 10 -- executed by `standby switchover`
- Step 11 -- executed by `standby switchover` because of the `--siblings-follow` option
- Step 12 -- executed by the script reacting to the `standby_switchover` event notification

Note that this sequence of events differs from the original one in two
aspects:

- Step 1 (performing a checkpoint on Node 1) is performed twice. This
  is not an issue; the second checkpoint will be essentially useless
  (and completely harmless) because it immediately follows the first
  checkpoint

- Step 9 is executed before Step 8, which is not a problem as these
  two steps don't depend on each other.

## Node Upgrade

In this section we describe how to perform software or hardware
upgrades on the cluster without interrupting the service.

We describe several procedures, depending on the nature of the upgrade
as well as on which node is being upgraded.

### Binary Compatible Upgrades

This procedure can be used to upgrade any hardware and software while
keeping binary compatibility of the PostgreSQL data directory.

We will only describe it on Standby nodes; to upgrade a Primary node,
it is sufficient to turn it into a Standby node with the [Switchover]
procedure.

Standby nodes 2 and 4 have a simpler upgrade as they have no
downstream node:

1. Stop PostgreSQL
2. Perform the upgrade
3. Start PostgreSQL. The Standby node will catch up with the WAL that
   was produced since Step 1. If that WAL is no longer available, the
   Standby will retrieve it from Barman.

Standby node 3 is streaming WAL to node 4 as a cascaded replica, but
it can be upgraded with the same procedure, provided that node 4 is
upgraded **first**.

Remember that this procedures are only possible for binary compatible
upgrades; it **cannot** be used for major PostgreSQL versions upgrades.

### Binary Incompatible Upgrades

\AuthorComment[Implementation Notes]{

"Provisioning a new cluster" means that we should provision four new
database nodes, including both PostgreSQL and repmgr; the existing
Barman and PgBouncer nodes can be reused, because their upgrade
procedure is different.

One issue to be solved is the fact that both the old and the new
repmgr instances are set up to reconfigure PgBouncer and Barman on
cluster role changes such as switchover or failover, and we must
verify that such reconfigurations do not interfere with each other.

For instance, the respective piece of configuration should be placed
in a per-cluster file, which is then included by the main Barman /
PgBouncer configuration. repmgr would then rewrite the file
corresponding to its own cluster, without affecting the configuration
of the other one.

From the application viewpoint, there will be four entry points: each
PgBouncer instance would expose two different entry points, one for
each cluster. Each entry point will be reflecting the state of the
corresponding cluster.

The procedure for the live switch to the new cluster must be carefully
designed; for instance, we could think of manually redirecting the old
entry points to the new cluster, and then gradually migrating the
application to the new entry points, but in the meantime only the new
entry points will be controlled by the new cluster, which could create
problems on failover.

}

If the upgrade is not binary compatible, e.g. from PostgreSQL 10 to
PostgreSQL 11, then we must follow a different strategy.

It is not possible to run different major versions of Physical
Streaming Replication inside the same cluster; therefore we will only
consider the case where we upgrade the whole PostgreSQL cluster.

The procedure we describe in this section is sketched in Figure
\ref{upgrade}, and is composed by the following steps:

![Binary Incompatible Upgrades\label{upgrade}](cache/upgrade.png){ width=85% }

1. Provision a new database cluster with upgraded operating system
   and/or software
2. Configure the new cluster as a separate ("New") server in both
   Barman nodes
3. Configure the new cluster as a separate ("New") database in both
   PgBouncer nodes
4. Set up Logical Replication using pglogical from the Primary node in
   the current PostgreSQL cluster to the Primary node of the new
   PostgreSQL cluster
5. Wait until the Logical Replication process is in sync
6. Perform a backup of the "New" server with Barman
7. Test applications on the new cluster, using the "New" entry point
8. If the test involved write transactions, the "New" database must be
   resynchronized with the "Old" one, invoking the
   `pglogical.alter_subscription_resynchronize_table()` function on
   all the tables that have been changed.
9. Redirect applications to use the new PostgreSQL cluster

\AuthorComment[TODO]{

In Step 8, can we use table statistics to check which tables have been
modified outside replication? It probably depends on whether SPI
writes are included in normal statistics, which we can find out by
carrying out a test on pglogical.

}

## Primary Node Failure

When the Primary node becomes unresponsive or unreachable, its state
must be monitored until it recovers, or until a predefined timeout.

If the Primary node recovers, the failure is proven to be temporary,
and the cluster returns to normal state without requiring any
action.

Conversely, if the Primary node does not recover within the timeout,
then it is considered permanently failed. On permanent failure a
Failover procedure is initiated, and the system must ensure that the
failed Primary does not return to the cluster.

The Failover procedure has the immediate goal of resuming write
activities, by promoting the "newest" standby to a primary node, and
the subsequent goal of restoring the expected redundancy, by replacing
the promoted standby node with a newly provisioned one.

Note that the old Primary node must be isolated from the application,
in order to prevent a *Split Brain* scenario which would require
extensive remedial work without guarantee of success.

Also, after a Failover procedure, the old Primary node is unable to
return to the replication cluster in the same capacity; it can only rejoin
the cluster as a Standby node, possibly after a "rewind" process that reverts
any unsent transaction.

It is therefore important to proceed with the Failover procedure only
after having firmly established that **the failed Primary is unable to
recover** in time to satisfy the cluster's requirements.

Assuming the initial configuration described by Figure \ref{ha} at
Page \pageref{ha}, the Failover procedure can be described as follows:

1. Primary Node 1 appears to fail
2. Confirm permanent failure of Node 1 (as discussed above)

![Primary Node Failure\label{ppf-1}](cache/ppf-1.png){
width=85% }

At this point, the cluster is in the state described in Figure
\ref{ppf-1}. In particular, both PgBouncer nodes are unable to perform
transactions as they are not connected to any PostgreSQL node.

We proceed with the following steps:

3. Reconfigure both PgBouncer instances to redirect traffic towards
   Node 2
4. Promote Node 2, which becomes the new Primary PostgreSQL node
5. Reconfigure the Primary Barman node to point to Node 2
6. Reconfigure the Standby PostgreSQL Node 3 to follow Node 2

![Primary Node Failure (Standby becomes new Primary)\label{ppf-2}](cache/ppf-2.png){
width=85% }

See Figure \ref{ppf-2} as reference of the cluster state after Step 6.

![Primary Node Failure (complete)\label{ppf-3}](cache/ppf-3.png){
width=85% }

7. Provision a new Standby Node 1' in Datacenter A, for instance
   using `repmgr` with the `standby clone` action, which can use
   a Barman backup when Barman mode is enabled
   (cfr. [Backups with Barman]), as shown in Figure \ref{ppf-3}.
8. Remove the failed node from the `repmgr` catalog using the 
   `primary unregister` action

After this procedure, Node 2 is promoted to be the new Primary
node, and a new standby Node 1' is created to restore the
planned fault tolerance.

Note that the failed Primary (Node 1) is completely isolated from any
other nodes, preventing a Split Brain
configuration, as noted above. In particular, it is isolated:

- from Application nodes, after Step 3
- from the Primary Barman node, after Step 5\
  (the Passive Barman node is not relevant as it does not connect to database nodes)
- from PostgreSQL nodes, after Step 8 

Note also that the failed Primary is **not** automatically reprovisioned as a
standby, which is intentional: a failed node requires proper
inspection before being reused, to establish whether the software and
the hardware are still viable.

### Reconfiguring Barman

In Step 5 of the Failover procedure, we redirect the Primary Barman
node towards Node 2.

The Primary Barman instance requires a replication slot to ensure that
no WAL is lost on temporary failures: we must therefore create a new
replication slot on Node 2, so that WAL streaming can resume.

## Standby Node Failure

In this section we discuss the case when one of the PostgreSQL Standby
Nodes fails permanently.

If the Standby node recovers, the cluster returns to normal state
without requiring any user action; in other words, the failure was
temporary.

Note that a standby can tolerate a very long disconnection before
falling behind: each WAL file is sent to the Primary Barman node, and
then relayed to the Passive Barman node.  Retention policy on both
Barman nodes is usually specified in terms of days or even weeks, and
PostgreSQL Standby nodes are able to seek WAL records from the local
Barman instance as well as from their upstream nodes.

Nevertheless, if the Standby node fails permanently, then a new
standby must be provisioned, for instance by restoring the latest
backup from Barman.

In the case of a permanent failure of Node 3, the procedure is
slightly more complex, because Node 3 is the upstream of Node 4, which
will also be disconnected. We recommend the following procedure:

1. Verify the permanent failure of Node 3
2. Reconfigure Node 4 to follow Primary Node 1
3. Remove the failed Node 3 from the `repmgr` catalog using the 
   `standby unregister` action
4. Create a new Standby Node 3' by restoring the latest backup from
   Barman
5. Reconfigure Node 3' so that it follows Node 4

Note that this operation will switch the cascading replication role of
nodes 3' and 4; if this is not desirable, once Node 3' is in sync with
its upstream, we can restore a configuration more similar to the
original one, as follows:

- Disconnect Node 4 and wait that 3' catches up
- Reconfigure Node 4 so it follows Node 3'
- Reconfigure Node 3' so it follows Node 1

## Barman Node Failure

On temporary failures of the Primary Barman node, the WAL accumulates
on the sending node, because the Barman streaming connection uses a
replication slot.

When the Barman node recovers, it can retrieve all missing WAL files,
and the cluster automatically returns to the original state.

In case of long downtime, however, WAL accumulation will eventually
fill the storage on the Primary PostgreSQL node, which will crash, and
will even be unable to recover until enough disk space is liberated.

Monitoring failures of the Primary Barman node and reacting soon
enough is therefore very important, even more so considering that the
Primary PostgreSQL Node would be at risk.

If the Primary Barman node fails and does not recover on time, we must
carry out the following procedure. To avoid confusion we will denote
Barman nodes as "A" or "B" depending on which datacenter they inhabit.

1. Drop Barman's replication slot on the Primary PostgreSQL node

2. Reconfigure Barman node B, which so far was a passive node, as a
   Primary PostgreSQL node directed to the current PostgreSQL Primary
   node
   
At this point, note that Barman is provisionally in the opposite
datacenter as its upstream database server.

3. Provision a new Barman node A' in datacenter A, as a passive node
   that points to the Barman node B.

4. Wait until Barman node A' is in sync with Barman node B

5. Reconfigure Barman node B' as passive, in sync with Barman node A'

6. Reconfigure Barman node A' as active, directed to the current
   Primary PostgreSQL node

If connectivity between datacenters cannot sustain cross-datacenter
Barman activity, it is sufficient to prepend the procedure with a
switchover from PostgreSQL Node 1 to PostgreSQL Node 3, so that the
traffic caused by Step 2 all occurs within Datacenter B.

The case when the Passive Barman node fails is much easier to
handle. Essentially:

1. Provision a new Passive Barman node in the same datacenter as the
   failed one; the new Passive Barman node will automatically begin a
   synchronization process from the Primary Barman node

2. Reconfigure the standby nodes in the same datacenter as the Passive
   Barman node so that they use the IP address of the new node when
   retrieving WAL from Barman

### Option: Single Barman

If the loss of recovery window is acceptable, it is possible to have
only the Primary Barman node.

![Option: Single Barman Node\label{ha-1b}](cache/1b.png){ width=85% }

The resulting architecture is described in Figure \ref{ha-1b}, and presents
the following critical elements:

- As we do not have a Barman node in datacenter 2, we lose PITR
  capability in case datacenter 1 fails
  
- Switchover and Failover procedures are slightly more complex because
  WAL data needs to cross datacenters to travel from the Primary
  PostgreSQL node to the only Barman node.

## PgBouncer Node Failure

The two PgBouncer nodes represent two independent *entry points* to
the PostgreSQL cluster, one for each datacenter.

Since the cluster has a single Primary Node, the main reason of having
two entry points is for redundancy. In particular, if one entry point
fails, then the application can just use the other one, without having
to perform any cluster-level procedure, as specified in Figure
\ref{ha-epf}.

![PgBouncer Node Failure\label{ha-epf}](cache/epf.png){ width=85% }

Examples of client failover syntax include JDBC, and LibPQ:

* <https://www.postgresql.org/docs/current/static/libpq-connect.html#LIBPQ-MULTIPLE-HOSTS>

For instance, when using LibPQ to specify a sequence of multiple entry
points, the application will react to connection failure by trying all
entry points in sequence until one of them succeeds.

### Alternate Failover Setups

Client Failover is one possible approach to ensuring that the cluster
does not have a single point of failure in its entry point.

While this approach has the merit of robustness without requiring
external components, we mention in this section two alternate
approaches that are commonly used:

- DNS Failover
- Virtual IP Failover

#### DNS Failover

DNS Failover relies on the name resolution layer for mapping a single
hostname to the preferred IP address. This is possible with an
appropriate configuration of the DNS server being used, by specifying
both PgBouncer nodes in the desired order of priority.

For instance, `entrypoint-a` can be resolved to the IP addresses of
PgBouncer A first and PgBouncer B second, while `entrypoint-b` can be
resolved to the same addresses in reverse order. Application servers
can use one name or the other depending on which node is preferred,
e.g. by their physical location.

In case of failure of one of the PgBouncer nodes, this configuration
will automatically redirect connections to the other PgBouncer node,
similarly to what happens with Client Failover.

The reliability of this solution depends on the reliability of the
name resolution layer, which can be achieved in a number of ways:

- configuring resolution statically on each application server
  (e.g. `/etc/hosts`);
- having two independent DNS servers;
- relying on an external DNS service, whose reliability is considered
  sufficient.

#### Virtual IP Failover

A virtual IP address can also be attached to each PgBouncer node, with
a pattern similar to the one described in [DNS Failover], that is:

- VirtualIP-A pointing to PgBouncer A
- VirtualIP-B pointing to PgBouncer B
- Applications using the "closest" Virtual IP

This solution needs an external component which reacts to changes in
the availability of PgBouncer nodes, such as failures, or nodes
becoming available again, and reconfigures the virtual IP as
appropriate.

We note that the robustness of this approach depends on the
reliability of this external component.

## Datacenter Failure

The failure of an entire datacenter can occur by several reasons,
including power outage, planned maintenance, or simply the lost of
connection between both datacenters, which is interpreted by each
datacenter as a failure of the other one. We must also distinguish
between the failure of a datacenter where the Primary Node is running,
in our case "Datacenter A", and the case were only Standby Nodes are running,
i.e. "Datacenter B".

### Failure of Datacenter B

Should Datacenter B fail, the application can still continue working
against the Primary PostgreSQL Node, because both this node and the
Standby PostgreSQL Node 2 are still functioning.

Application connections will migrate gently: any application that was
connected to the PgBouncer instance in Datacenter B will have been
disconnected, and will have retried to reconnect via the PgBouncer
instance in Datacenter A.

Once those two actions has been taken, a new Datacenter B' must be
provisioned, with the same component layout:

1. New PgBouncer, as a clone of PgBouncer node in Datacenter A
2. New Passive Barman node B', synchronizing from the Barman node A
3. New Standby Nodes 3' and 4', built by restoring a backup from the
   new Passive Barman node B' (once it is in sync)
4. Standby Node 4' must be configured to follow Standby Node 3',
   according to the original cascading replication scheme.

### Failure of Datacenter A

If Datacenter A fails, then the only remaining option is failing over
to Datacenter B.

Note that, under our architecture, application nodes are running in a
separate datacenter.

The failover procedure to Datacenter B, in case of total loss of
Datacenter A, is presented in Figure \ref{dc-failure}:

![Datacenter Failure\label{dc-failure}](cache/dcf.png){ width=85% }

1. Verify that Datacenter A is failed, and that Datacenter B is
   functioning
2. All applications that were previously connected to the PgBouncer
   node in Datacenter A must be redirected to the PgBouncer Node on
   Datacenter B
3. Promote Node 3 in Datacenter B to become the new Primary Node
4. Reconfigure the Passive Barman node in Datacenter B as a Primary
   Barman node directed to Node 3

After this procedure, applications are reconfigured to use Datacenter
B, as Figure \ref{dc-failure-2} shows.

![Recovery from Datacenter Failure\label{dc-failure-2}](cache/dcf2.png){ width=85% }

The procedure just described is able to restore operations after
failure of the datacenter where the Primary node is running. However,
the resulting cluster does not have full redundancy capabilities,
because it is vulnerable to a second Datacentre loss. Such issue must
therefore be addressed at the earliest opportunity by provisioning new
nodes in a new, functioning datacenter.

This procedure is very similar to the one described in [Failure of
Datacenter B], because it adds new Standby and passive nodes to the
cluster, according to the following layout:

1. New PgBouncer node in Datacenter A', with the same configuration as
   the PgBouncer node in Datacenter B
2. New Passive Barman node, synchronizing from the Barman node in
   Datacenter B
2. New Standby Nodes 1' and 2', obtained by restoring a backup from
   the Passive Barman node in the same datacenter;
3. Standby Node 2' follows Standby Node 1', which in turn follows Node
   3, the current Primary PostgreSQL node.

Finally, note that this procedures switches the roles of Datacenter A
and Datacenter B, in terms of which one hosts the Primary PostgreSQL
node.
