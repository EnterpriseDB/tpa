---
title: "Trusted PostgreSQL Architecture: M1"
version: 1.2
customer: 2ndQuadrant
copyright-years: 2018
date: 28 December 2018
author: 2ndQuadrant
table-row-colors: false
toc: true
classification: internal
---

\BeginRevisions

\Revision 1.0; 2018-05-17; BM, GC: First version.

\Revision 1.1; 2018-12-17; GC: Mentioning alternate approaches on
PgBouncer node failure.

\Revision 1.2; 2018-12-28; GC: Introducing the "Two Barman, Local
Source" WIP scenario.

\EndRevisions


# Introduction

In this document we describe a High Availability architecture based on
PostgreSQL.

# Database Architecture

![Database Architecture Diagram\label{ha}](cache/ha.png){ width=85% }

The architecture we propose is displayed in Figure \ref{ha}.

This architecture presents the following characteristics:

- It includes the following components:
    - Four PostgreSQL nodes (nodes 1 - 4)
        - One node designated as Primary
        - Three nodes designated as Standby
    - Two PgBouncer nodes
    - One Barman node
    - Each PostgreSQL node runs repmgr
- It is based on the following software:
    - PostgreSQL 10
    - PgBouncer 1.9
    - repmgr 4
    - Barman 2.4
- Applications access the database layer through PgBouncer
- Standby nodes 2, 3, 4 are cloned from Node 1 using repmgr, and can
  replace Node 1 on failure
- The Barman node is used for taking backups from the Primary
- WAL is transmitted using *Physical Streaming Replication* (PSR):
    - from Primary to Standby nodes 2, 3
	- from Primary to the Barman node
    - from Node 3 to Node 4, both located on the same datacenter
- Barman is using a *replication slot*, to ensure that upstream
  WAL is received before being eliminated
    - Standby nodes do not use replication slots; if they fall behind
      due to WAL streaming being disrupted, they can recover any
      missing WAL directly from the Barman node.

## Backups with Barman

The database is backed up using Barman, which is initially configured
to perform base backups and archive WAL files from Node 1, the Primary
node in Datacenter A.

The base backup, together with the WAL archive, can be used to restore
the following:

- Restore a PostgreSQL node using Point-In-Time-Recovery
- Provide Standby nodes with any older WAL files that have already
  been eliminated from their upstream nodes.

\AuthorComment[GC]{

We choose WAL streaming because it provides a lower RPO and it is
safer than the alternative of using log shipping through rsync.

The related failure scenario, as pointed out by Álvaro Herrera, is the
Barman server crashing after the WAL file is transferred by rsync, but
before it is flushed to disk. In this case the WAL file could be
missing from Barman's catalog, despite being successfully archived.

WAL streaming is not affected because it uses pg\_receivewal, which
flushes the WAL file to disk and transmits the corresponding flush LSN
to the upstream node.

In some cases log shipping would still provide some advantages
(e.g. parallel archiving) but for now we don't consider it as an
option.

}

# High Availability Procedures

In this section we describe High Availability procedures in more
detail, including failure scenarios as well as maintenance
activities. We cover the following procedures:

- [Switchover]
- [Node Upgrade]
- [Primary Node Failure]
- [Standby Node Failure]
- [Barman Node Failure]
- [PgBouncer Node Failure]
- [Datacenter Failure]

## Switchover

Note that the *Switchover* procedure is a planned intervention, as
opposed to the *Failover* procedure described in
[Primary Node Failure], designed to react to an unplanned failure.

The switchover procedure from Primary (Node 1) to a standby in the
same datacenter (Node 2) is as follows:

1. Perform a `CHECKPOINT` on the Primary Node 1
2. Pause PgBouncer
3. Shut down Node 1
4. Wait until Node 2 has replayed all the WAL received from Node 1
5. Promote Node 2 to Primary
6. Reconfigure PgBouncer to point to Node 2 (the new Primary)
7. Resume PgBouncer
8. Reconfigure Standby Node 3 to follow the new Primary node
9. Reconfigure Barman to point to the new Primary node


## Node Upgrade

In this section we describe how to perform software or hardware
upgrades on the cluster without interrupting the service.

We describe several procedures, depending on the nature of the upgrade
as well as on which node is being upgraded.

### Binary Compatible Upgrades

This procedure can be used to upgrade any hardware and software while
keeping binary compatibility of the PostgreSQL data directory.

We will only describe it on Standby nodes; to upgrade a Primary node,
it is sufficient to turn it into a Standby node with the [Switchover]
procedure.

Standby nodes 2 and 4 have a simpler upgrade as they have no
downstream node:

1. Stop PostgreSQL
2. Perform the upgrade
3. Start PostgreSQL. The Standby node will catch up with the WAL that
   was produced since Step 1. If that WAL is no longer available, the
   Standby will retrieve it from Barman.

The procedure to upgrade Standby node 3 is as follows:

1. Reconfigure Node 4 to connect directly to the Primary node (i.e. to
   "follow" the Primary node) instead of Standby node 3
2. Stop PostgreSQL on the Standby node 3
3. Perform the upgrade on the Standby node 3
4. Reconfigure Node 3 to follow the Standby node 4
5. Start the Standby node 3

Note that this operation will switch the cascading replication role of
nodes 3 and 4. This is not an issue as they have equivalent roles, but
if desired the original roles can be restored by replacing Step 4 and
5 with the following:

4. Reconfigure Node 3 to follow the Primary node
5. Reconfigure Node 4 to follow the Standby node 3

Remember that these procedures are only possible for binary compatible
upgrades; they **cannot** be used for major PostgreSQL versions
upgrades

### Binary Incompatible Upgrades

If the upgrade is not binary compatible, e.g. from PostgreSQL 10 to
PostgreSQL 11, then we must follow a different strategy.

It is not possible to run different major versions of Physical
Streaming Replication inside the same cluster; therefore we will only
consider the case where we upgrade the whole PostgreSQL cluster.

A PostgreSQL cluster can be upgraded as follows:

1. Provision a new PostgreSQL cluster with upgraded operating system
   and/or software, with the same layout as described in Figure
   \ref{ha}
2. Set up Logical Replication using pglogical from the Primary node in
   the current PostgreSQL cluster to the Primary node of the new
   PostgreSQL cluster
3. Wait until the Logical Replication process has caught up
4. In the new PostgreSQL cluster, perform a backup with Barman
5. Perform application testing as required on the new PostgreSQL
   cluster
6. Redirect the application so that it uses the new PostgreSQL cluster

## Primary Node Failure

When the Primary node becomes unresponsive or unreachable, its state
must be monitored until it recovers, or until it times out in
unreachable state. If the Primary node recovers before a predefined
timeout, we categorize the event as a temporary failure, and the
situation is back to normal without taking any action. If the Primary
node does not recover, it is considered permanently failed upon the
timeout. The permanent failure triggers the Failover procedure, and
the system must ensure that the failed Primary does not return to the
cluster.

Should the Primary node fail, we can initiate a Failover procedure,
with the immediate goal of restoring write capabilities, by promoting
the "nearest" standby to a primary node, and the subsequent goal of
restoring the expected redundancy by replacing the promoted standby
with a newly provisioned node.

Note that the old Primary node must be isolated from the application,
in order to prevent a "Split Brain" scenario which would require
extensive remedial work without guarantee of success.

Also, after a Failover procedure, the old Primary node is unable to
join the replication cluster; it can only rejoin as a Standby after a
"rewind" procedure, which will undo any unsent transaction.

Hence it is important to proceed with the Failover procedure only
after having established that **the failed Primary is unable to
recover** in time to satisfy the cluster's requirements.

Assuming the initial configuration described by Figure \ref{ha} at
Page \pageref{ha}, the Failover procedure can be described as follows:

1. Primary Node 1 appears to fail
2. Confirm permanent failure of Node 1 (as discussed above)

![Primary Node Failure\label{ppf-1}](cache/ppf-1.png){
width=85% }

At this point, the cluster is in the state described in Figure
\ref{ppf-1}. In particular, both PgBouncer nodes are unable to perform
transactions as they are not connected to any PostgreSQL node.

We proceed with the following steps:

3. Reconfigure both PgBouncer instances to redirect traffic towards
   Node 2
4. Promote Node 2, which becomes the new Primary node
5. Reconfigure Barman to point to Node 2, the new Primary Node
6. Reconfigure Standby 3 to follow Node 2, the new Primary Node

![Primary Node Failure (Swap to Standby -> New Primary)\label{ppf-2}](cache/ppf-2.png){
width=85% }

See Figure \ref{ppf-2} as reference of the current state.

![Primary Node Failure (complete)\label{ppf-3}](cache/ppf-3.png){
width=85% }

7. Provision a new Standby Node 1' in Datacenter A, for instance
   using `repmgr` to clone a standby from a Barman backup
   (cfr. [Backups with Barman]), as shown in Figure \ref{ppf-3}.

After this procedure, Node 2 has been promoted to be the new Primary
node, and a new standby Node 1' has been created to restore the
planned fault tolerance.

Note that the failed Primary (Node 1) is completely isolated from the
application, which as explained above prevents a Split Brain
configuration.

Also, the failed Primary is not automatically reprovisioned as a
standby, which is intentional, since a failed node requires proper
inspection before being reused, as it could not be feasible depending
on the nature of the failure.

### Reconfiguring Barman

In Step 5 of the Failover procedure, we reconfigure Barman to follow
Node 2, the new Primary Node. Barman uses a replication slot to ensure
that no WAL is lost on temporary failures. That means that Step 5
includes recreating Barman's replication slot on the new Primary Node.
Once Barman has been reconfigured, backups can continue with the same
schedule.

## Standby Node Failure

In this section we discuss the case when Nodes 2, 3, and 4 fail
permanently.

If the node recovers soon enough, before a predefined timeout, we are
in the event of a temporary failure, where the system recovers
automatically without any operator intervention.

Note that while a standby node is unavailable, WAL files will be still
sent from Primary to Barman. When the Standby node returns, it will be
able to retrieved WAL files from Barman if they have been eliminated
already by the Primary node.

Should the Standby node 2 or 4 fail permanently, a new standby (say
node 2' or 4') can be provisioned by restoring the latest backup from
Barman.

In the case of a permanent failure of Standby node 3, the procedure is
slightly different, because Node 3 is the upstream of Node 4. In such
case, the procedure to follow is:

1. A permanent failure of Node 3 has been detected
2. Reconfigure Node 4 to follow Primary node 1
3. Create a new Standby Node 3' restoring the latest backup from
   Barman
4. Make Node 3' follow Node 4

Note that this operation will switch the cascading replication role of
nodes 3 and 4, similarly to what is described in
[Binary Compatible Upgrades].


## Barman Node Failure

Barman uses a replication slot to ensure that no WAL is lost on
temporary failures. When the Barman node recovers rapidly, the cluster
automatically goes back to the original state, without need for any
human intervention.  However, the use of the replication slot means
that WAL is accumulated on the sending node when the Barman node is
down. In this architecture, as shown in Figure \ref{ha}, the sending
is the Primary Node. This means that a prolonged Barman downtime
affects the Primary node, and should therefore be considered permanent
soon enough.

If the Barman node fails to recover, we must carry out the following
procedure:

1. Provision a new Barman node

	a. In addition to the standard Barman configuration, the
       replication slot must be reset to the current position, using
       the `--reset` option of the `barman receive-wal` command. This
       is because the replication slot exists already and is pointing
       to the location reached by the previous Barman server

2. Take a backup to verify that the configuration is functioning

This will not affect the other nodes, but the desired recovery window
will not be immediately available: the old backups have been lost
together with the failed Barman node, so that the oldest starting
point for recovery will be the end of the backup taken at Step 2.

For instance, even if a retention policy stating `RECOVERY WINDOW OF 1
WEEK` is specified, it will be necessary to wait at least one week
before that window is available.

### Barman Redundancy: Same Source

If the loss of recovery window is unacceptable, an extra Barman node
can be created in the other datacenter.

![Option: Two Barman Nodes, Same Source\label{ha-2b}](cache/2b.png){ width=85% }

The resulting architecture is described in Figure \ref{ha-2b}: the two
Barman nodes operate independently of each other, meaning that no
particular coordination between them is required.

Note that both Barman node are configured to point to the current
primary node, and they are both subject to the remarks in the
[Reconfiguring Barman] section.

Thanks to the Point-In-Time Recovery (PITR) technology, provided by
PostgreSQL, each backup can be used to restore any subsequent database
states by applying WAL records as appropriate.

This option has the advantage of providing full PITR capabilities
under all the current failure scenarios.

### Barman Redundancy: Local Source (WIP)

![Option: Two Barman Nodes, Local Source (WIP)\label{ha-2bl}](cache/2bl.png){
 width=85% }

In Figure \ref{ha-2bl} we consider an alternate approach, where each
Barman node is pointed to a PostgreSQL node in the same datacenter,
respectively Node 1 and Node 3.

Note that, as of version 2.5, Barman supports base backups from a
standby node, but WAL files must be received from the primary node.

In particular, this configuration is **not officially supported**; we
discuss it for facilitating the discussion of whether it is desirable
to have this option, and of what changes we need to apply to Barman
and/or to the cluster configuration in order to make it work.

## PgBouncer Node Failure

The two PgBouncer nodes represent two independent *entry points* to
the PostgreSQL cluster, one for each datacenter.

Since the cluster has a single Primary Node, the main reason of having
two entry points is for redundancy. In particular, if one entry point
fails, then the application can just use the other one, without having
to perform any cluster-level procedure, as specified in Figure
\ref{ha-epf}.

![PgBouncer Node Failure\label{ha-epf}](cache/epf.png){ width=85% }

Examples of client failover syntax include JDBC, and LibPQ:

* <https://www.postgresql.org/docs/10/static/libpq-connect.html#LIBPQ-MULTIPLE-HOSTS>

For instance, when using LibPQ to specify a sequence of multiple entry
points, the application will react to connection failure by trying all
entry points in sequence until one of them succeeds.

### Alternate Failover Setups

Client Failover is one possible approach to ensuring that the cluster
does not have a single point of failure in its entry point.

While this approach has the merit of robustness without requiring
external components, we mention in this section two alternate
approaches that are commonly used:

- DNS Failover
- Virtual IP Failover

#### DNS Failover

DNS Failover relies on the name resolution layer for mapping a single
hostname to the preferred IP address. This is possible with an
appropriate configuration of the DNS server being used, by specifying
both PgBouncer nodes in the desired order of priority.

For instance, `entrypoint-a` can be resolved to the IP addresses of
PgBouncer A first and PgBouncer B second, while `entrypoint-b` can be
resolved to the same addresses in reverse order. Application servers
can use one name or the other depending on which node is preferred,
e.g. by their physical location.

In case of failure of one of the PgBouncer nodes, this configuration
will automatically redirect connections to the other PgBouncer node,
similarly to what happens with Client Failover.

The reliability of this solution depends on the reliability of the
name resolution layer, which can be achieved in a number of ways:

- configuring resolution statically on each application server
  (e.g. `/etc/hosts`);
- having two independent DNS servers;
- relying on an external DNS service, whose reliability is considered
  sufficient.

#### Virtual IP Failover

A virtual IP address can also be attached to each PgBouncer node, with
a pattern similar to the one described in [DNS Failover], that is:

- VirtualIP-A pointing to PgBouncer A
- VirtualIP-B pointing to PgBouncer B
- Applications using the "closest" Virtual IP

This solution needs an external component which reacts to changes in
the availability of PgBouncer nodes, such as failures, or nodes
becoming available again, and reconfigures the virtual IP as
appropriate.

We note that the robustness of this approach depends on the
reliability of this external component.

## Datacenter Failure

The failure of an entire datacenter can occur by several reasons,
including power outage, planned maintenance, or simply the lost of
connection between both datacenters, which is interpreted by each
datacenter as a failure of the other one. We must also distinguish
between the failure of a datacenter where the Primary Node is running,
in our case "DC A", and the case were only Standby Nodes are running,
i.e. "DC B".

### Failure of Datacenter B

In case of failure of "DC B", the application can still continue
working as the Primary Node, and Standby Node 2 are still working
properly. The first actions that need to be taken are the following:

1. Applications that were connected to PgBouncer in DC B need to
   reconnect through PgBouncer in DC A
2. In case Barman is not redundant, create a new Barman node in DC
   A. This is specially because recovering an entire datacenter might
   take a considerable amount of time. See section
   [Barman Node Failure] for more details

Once those two actions has been taken, create a new Datacenter DC B'
with the same components:

1. New PgBouncer as a clone of PgBouncer node in DC A
2. New Standby Nodes 3' and 4', built by restoring a backup from the
   new Barman node running in DC A
3. Configure Standby Node 4' to follow Standby Node 3' to return to
   the original cascading replication scheme
4. Alternatively, create a new Barmana Node at DC B' for redundancy

### Failure of Datacenter A

If datacenter DC A fails, then the only remaining option is failing
over to datacenter DC B. If the datacenter is fully lost, the
application nodes running in DC A will also be inoperable. In this
procedure we will consider the case where applications connected to DC
A, are running on a different datacenter that is still working.

This is the procedure to failover to Datacenter B in the case of total
loss of Datacenter A:

1. Verify that DC A is failed, and that DC B is functioning
2. Redirect the application connected to DC A towards PgBouncer Node
   on DC B
3. Promote Node 3 in DC B to become the new Primary Node
4. Reconfigure Barman to follow Node 3, the new Primary

After this procedure, applications are reconfigured to use Datacenter
B, as Figure \ref{dc-failure} shows.

![Datacenter Failure\label{dc-failure}](cache/dcf2.png){ width=85% }

The procedure just described is able to restore operations after
failure of the datacenter where the Primary node is running. However,
the resulting cluster does not have full redundancy capabilities,
because it is vulnerable to a second Datacentre loss. Such issue must
therefore be addressed at the earliest opportunity by provisioning new
nodes in another functioning datacenter. This procedure is very
similar to the one described in [Failure of Datacenter B], because it
builds Standby nodes.

1. New PgBouncer in DC A' as a clone of PgBouncer node in DC B
2. New Standby Nodes 1' and 2', built by restoring a backup from
   Barman running in DC B
3. Configure Standby Node 2' to follow Standby Node 1' to create a
   cascading replication scheme
4. Alternatively, create a new Barmana Node at DC A' for redundancy

Note that this procedures switches the roles of DC A and DC B, having
the Primary node recovered at DC B.
