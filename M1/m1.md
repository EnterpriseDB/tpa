---
title: "Trusted PostgreSQL Architecture: M1"
version: 2.0 (DRAFT 2)
customer: 2ndQuadrant
copyright-years: 2018-19
date: 3 February 2019
author: 2ndQuadrant
table-row-colors: false
toc: true
classification: internal
display-author-comments: true
display-revision-history: false
---

\BeginRevisions

\Revision 1.0; 2018-05-17; BM, GC: First version.

\Revision 1.1; 2018-12-17; GC: Mentioning alternate approaches on
PgBouncer node failure.

\Revision 1.2; 2018-12-28; GC: Introducing the "Two Barman, Local
Source" WIP scenario.

\Revision 1.3; 2018-12-28; GC: Updated version numbers of various
software.

\Revision 1.4; 2019-01-18; GC: Replaced WIP section with a comment, so
the file can be shared with customers.

\Revision 1.5; 2019-01-18; GC: Fixed some inaccuracies in graphs (all
repmgrs interact with all pgbouncers, irrespective of location).

\Revision 2.0 (DRAFT 2); 2019-02-02; GC: Introducing a second Barman
node; many failure scenarios reworked as appropriate.

\Revision 2.0 (DRAFT 3); 2019-02-11; GC: Using the newly available
Geo-redundancy for the second Barman node.

\EndRevisions


\AuthorComment[TODO]{

We should consider introducing Barman's Geo-redundancy mode: configure
the Barman node in the primary DC as active (i.e. reading from
PostgreSQL), and the other Barman node as passive (i.e. reading from
the active node). The standby Barman will then be used by all its
local PostgreSQL nodes as a source for older WAL.

In case of failover/switchover to a node in the opposite DC, we also
need to carry out a "Barman switchover" process, which should
not be difficult, but requires some investigation to be done safely,
as it's not covered by Barman docs.

The "Barman node failure" scenario also needs a "Barman failover"
process. One hopes that this is less critical than database failover,
due to the incremental nature of Barman's data files.

This mode should optionally work even in conjunction with shared
filesystem, due to the reduced requirements of a passive Barman
node. This also requires some analysis.

}

# Introduction

In this document we describe a High Availability architecture based on
PostgreSQL.

## Database Architecture

![Database Architecture Diagram\label{ha}](cache/ha.png){ width=85% }

The architecture is displayed in Figure \ref{ha}, and presents the
following characteristics:

- It is deployed over two datacenters
- Each datacenter is an *availability zone*
- The following components are included:
    - Four PostgreSQL nodes (nodes 1 - 4)
        - One node designated as Primary
        - Three nodes designated as Standby
    - Two PgBouncer nodes
    - Two Barman nodes
        - The node in the same datacenter as the Primary PostgreSQL
          node is configured as Primary
        - The one in the other datacenter is configured as Passive
    - Each PostgreSQL node runs repmgr
- The following software is used:
    - PostgreSQL 11
    - PgBouncer 1.9
    - repmgr 4.2
    - Barman 2.6
- Applications access the database layer through PgBouncer
- PostgreSQL Standby nodes 2, 3, 4 are cloned from PostgreSQL Node 1
  using repmgr, and can replace Node 1 on failure
- The Primary Barman node takes backups from the Primary PostgreSQL
  node
- WAL is transmitted using *Physical Streaming Replication* (PSR):
    - from the Primary PostgreSQL node to Standby PostgreSQL nodes 2, 3
	- from the Primary PostgreSQL node to the Primary Barman node
    - from Node 3 to Node 4, both located on the same datacenter
- The Primary Barman node uses a *replication slot* to ensure that the
  upstream node does not remove unsent WAL
- The Passive Barman node fetches WAL and backups from the Primary
  Barman node using Geo-redundancy mode
- Standby PostgreSQL nodes do not use replication slots; if they fall
  behind due to WAL streaming being disrupted, they can restore any
  missing WAL directly from the local Barman node

\AuthorComment[GC]{

TPAexec is currently forcing version 4.0.6 of repmgr because of an
incompatibility with repmgr 4.1+; the corresponding Redmine issue is
https://redmine.2ndquadrant.com/issues/4931

}

## Backups with Barman

The database is backed up using the Primary Barman node, which is
configured to perform base backups and archive WAL files from Node 1
in Datacenter A, which is initially the Primary node.

The base backup, together with the WAL archive, can be used as
follows:

- To restore a PostgreSQL node using Point-In-Time-Recovery
- To provide Standby nodes with any older WAL files that have already
  been eliminated from their upstream nodes.

\AuthorComment[GC]{

We choose WAL streaming because it provides a lower RPO and it is
safer than the alternative of using log shipping through rsync.

The related failure scenario, as pointed out by Ãlvaro Herrera, is the
Barman server crashing after the WAL file is transferred by rsync, but
before it is flushed to disk. In this case the WAL file could be
missing from Barman's catalog, despite being successfully archived.

WAL streaming is not affected because it uses pg\_receivewal, which
flushes the WAL file to disk and transmits the corresponding flush LSN
to the upstream node.

In some cases log shipping would still provide some advantages
(e.g. parallel archiving) but for now we don't consider it as an
option.

}

The Passive Barman nodes does not connect to PostgreSQL; it fetches
data from the Primary Barman node, to provide backup redundancy
without additional I/O pressure on the database servers.

# High Availability Procedures

In this section we describe High Availability procedures in more
detail, including failure scenarios as well as maintenance
activities. We cover the following procedures:

- [Switchover]
- [Node Upgrade]
- [Primary Node Failure]
- [Standby Node Failure]
- [Barman Node Failure]
- [PgBouncer Node Failure]
- [Datacenter Failure]

## Switchover

Note that the *Switchover* procedure is a planned intervention, as
opposed to the *Failover* procedure described in
[Primary Node Failure], designed to react to an unplanned failure.

The switchover procedure from Primary (Node 1) to a standby in the
same datacenter (Node 2) is as follows:

1. Perform a `CHECKPOINT` on the Primary Node 1
2. Pause PgBouncer

At this point the database service is interrupted.

3. Shut down Node 1
4. Wait until Node 2 has replayed all the WAL received from Node 1
5. Promote Node 2 to Primary
6. Reconfigure PgBouncer to point to Node 2 (the new Primary)
7. Resume PgBouncer

At this point the service is restored.

8. Reconfigure Node 1 as a Standby node using the `repmgr node rejoin`
   syntax
9. Reconfigure Standby Node 3 to follow the new Primary node using
   `repmgr standby follow`
10. Reconfigure the Primary Barman node to point to Node 2, the new
    PostgreSQL Primary node

\AuthorComment{

We only have one kind of database connection defined in
PgBouncer. However, we can improve read-only availability by setting
up a second PgBouncer database and pointing it to one of the standbys.

This assumes that the application is able to separate reads from
writes.

}

### Using `repmgr standby switchover`

If appropriately configured, `repmgr` allows a simplified switchover
procedure with its dedicated syntax, as follows:

i. Perform a `CHECKPOINT` on the Primary Node 1
ii. Pause PgBouncer
iii. Execute `repmgr standby switchover --siblings-follow`

The simplified switchover procedure is related to the former switchover procedure as follows:

- Step "i" is the same as Step 1
- Step "ii" is the same as Step 2
- Step "iii" includes steps 1,3,4,5,8
- Step "iii" also includes step 9, thanks to the `--siblings-follow` option
- Steps 6,7 can be implemented as part of a script reacting to the
  `standby_promote` event notification
- Step 10 can be implemented as part of a script reacting to the
  `standby_switchover` event notification

In particular, the resulting sequence of events is:

- Step 1 -- explicitely executed
- Step 2 -- explicitely executed
- Step 1 again -- executed by `standby switchover`
- Steps 3, 4, 5 -- executed by `standby switchover`
- Steps 6, 7 -- executed by the script reacting to the `standby_promote` event notification
- Step 8 -- executed by `standby switchover`
- Step 9 -- executed by `standby switchover` because of the `--siblings-follow` option
- Step 10 -- executed by the script reacting to the `standby_switchover` event notification

Note that Step 1 (performing a checkpoint on Node 1) is performed
twice. This is not an issue; the second checkpoint will be essentially
useless (and completely harmless) because it immediately follows the
first checkpoint.

## Node Upgrade

In this section we describe how to perform software or hardware
upgrades on the cluster without interrupting the service.

We describe several procedures, depending on the nature of the upgrade
as well as on which node is being upgraded.

### Binary Compatible Upgrades

This procedure can be used to upgrade any hardware and software while
keeping binary compatibility of the PostgreSQL data directory.

We will only describe it on Standby nodes; to upgrade a Primary node,
it is sufficient to turn it into a Standby node with the [Switchover]
procedure.

Standby nodes 2 and 4 have a simpler upgrade as they have no
downstream node:

1. Stop PostgreSQL
2. Perform the upgrade
3. Start PostgreSQL. The Standby node will catch up with the WAL that
   was produced since Step 1. If that WAL is no longer available, the
   Standby will retrieve it from Barman.

Standby node 3 is streaming WAL to node 4 as a cascaded replica, but
it can be upgraded with the same procedure, provided that node 4 is
upgraded **first**.

Remember that this procedures are only possible for binary compatible
upgrades; it **cannot** be used for major PostgreSQL versions upgrades.

### Binary Incompatible Upgrades

\AuthorComment[GC]{

Here we need to be more specific: provisioning a new cluster means
that we should provision four new database nodes, with PostgreSQL and
repmgr; the existing Barman and PgBouncer nodes can be reused, because
their upgrade procedure is different.

One issue to be solved is the fact that both the old and the new
repmgr instances are set up to reconfigure PgBouncer and Barman on
cluster role changes such as switchover or failover, and we must
verify that such reconfigurations do not interfere with each other.

For instance, the respective piece of configuration should be placed
in a per-cluster file, which is then included by the main Barman /
PgBouncer configuration. repmgr would then rewrite the file
corresponding to its own cluster, without affecting the configuration
of the other one.

From the application viewpoint, there will be four entry points: each
PgBouncer instance would expose two different entry points, one for
each cluster. Each entry point will be reflecting the state of the
corresponding cluster.

The procedure for the live switch to the new cluster must be carefully
designed; for instance, we could think of manually redirecting the old
entry points to the new cluster, and then gradually migrating the
application to the new entry points, but in the meantime only the new
entry points will be controlled by the new cluster, which could create
problems on failover.

}

If the upgrade is not binary compatible, e.g. from PostgreSQL 10 to
PostgreSQL 11, then we must follow a different strategy.

It is not possible to run different major versions of Physical
Streaming Replication inside the same cluster; therefore we will only
consider the case where we upgrade the whole PostgreSQL cluster.

The procedure we describe in this section is sketched in Figure
\ref{upgrade}, and is composed by the following steps:

![Binary Incompatible Upgrades\label{upgrade}](cache/upgrade.png){ width=85% }

1. Provision a new database cluster with upgraded operating system
   and/or software
2. Configure the new cluster as a separate ("New") server in both
   Barman nodes
3. Configure the new cluster as a separate ("New") database in both
   PgBouncer nodes
4. Set up Logical Replication using pglogical from the Primary node in
   the current PostgreSQL cluster to the Primary node of the new
   PostgreSQL cluster
5. Wait until the Logical Replication process is in sync
6. Perform a backup of the "New" server with Barman
7. Test applications on the new cluster, using the "New" entry point
8. If the test involved write transactions, the "New" database must be
   resynchronized with the "Old" one, invoking the
   `pglogical.alter_subscription_resynchronize_table()` function on
   all the tables that have been changed.
9. Redirect applications to use the new PostgreSQL cluster

\AuthorComment[TODO]{

In Step 8, can we use table statistics to check which tables have been
modified outside replication? It probably depends on whether SPI
writes are included in normal statistics, which we can find out by
carrying out a test on pglogical.

}

## Primary Node Failure

When the Primary node becomes unresponsive or unreachable, its state
must be monitored until it recovers, or until a predefined timeout.

If the Primary node recovers, the failure is proven to be temporary,
and the cluster returns to normal state without requiring any
action.

Conversely, if the Primary node does not recover within the timeout,
then it is considered permanently failed. On permanent failure a
Failover procedure is initiated, and the system must ensure that the
failed Primary does not return to the cluster.

The Failover procedure has the immediate goal of resuming write
activities, by promoting the "newest" standby to a primary node, and
the subsequent goal of restoring the expected redundancy, by replacing
the promoted standby node with a newly provisioned one.

Note that the old Primary node must be isolated from the application,
in order to prevent a *Split Brain* scenario which would require
extensive remedial work without guarantee of success.

Also, after a Failover procedure, the old Primary node is unable to
return to the replication cluster in the same capacity; it can only be
join as a Standby node, possibly after a "rewind" process that reverts
any unsent transaction.

It is therefore important to proceed with the Failover procedure only
after having firmly established that **the failed Primary is unable to
recover** in time to satisfy the cluster's requirements.

Assuming the initial configuration described by Figure \ref{ha} at
Page \pageref{ha}, the Failover procedure can be described as follows:

1. Primary Node 1 appears to fail
2. Confirm permanent failure of Node 1 (as discussed above)

![Primary Node Failure\label{ppf-1}](cache/ppf-1.png){
width=85% }

At this point, the cluster is in the state described in Figure
\ref{ppf-1}. In particular, both PgBouncer nodes are unable to perform
transactions as they are not connected to any PostgreSQL node.

We proceed with the following steps:

3. Reconfigure both PgBouncer instances to redirect traffic towards
   Node 2
4. Promote Node 2, which becomes the new Primary node
5. Reconfigure Barman nodes to point to Node 2, the new Primary Node
6. Reconfigure Standby 3 to follow Node 2, the new Primary Node

![Primary Node Failure (Swap to Standby -> New Primary)\label{ppf-2}](cache/ppf-2.png){
width=85% }

See Figure \ref{ppf-2} as reference of the current state.

![Primary Node Failure (complete)\label{ppf-3}](cache/ppf-3.png){
width=85% }

7. Provision a new Standby Node 1' in Datacenter A, for instance
   using `repmgr` with the `standby clone` action, which can use
   a Barman backup when Barman mode is enabled
   (cfr. [Backups with Barman]), as shown in Figure \ref{ppf-3}.
8. Remove the failed node from the `repmgr` catalog using the 
   `primary unregister` action

After this procedure, Node 2 is promoted to be the new Primary
node, and a new standby Node 1' is created to restore the
planned fault tolerance.

Note that the failed Primary (Node 1) is completely isolated from any
other nodes, preventing a Split Brain
configuration, as noted above. In particular, it is isolated:

- from Application nodes, after Step 3
- from Barman nodes, after Step 5
- from PostgreSQL nodes, after Step 8 

Note also that the failed Primary is **not** automatically reprovisioned as a
standby, which is intentional, since a failed node requires proper
inspection before being reused, as it could not be feasible depending
on the nature of the failure.

### Reconfiguring Barman

In Step 5 of the Failover procedure, we reconfigure Barman nodes to follow
Node 2, the new Primary Node.
Each Barman instance uses a replication slot to ensure
that no WAL is lost on temporary failures. That means that Step 5
includes recreating Barman's replication slots on the new Primary Node.
Once Barman nodes have been reconfigured, backups can continue with the same
schedule.

## Standby Node Failure

In this section we discuss the case when Nodes 2, 3, and 4 fail
permanently.

If the Standby node recovers, the cluster returns to normal state
without requiring any user action; in other words, the failure was
temporary.

Note that a standby can tolerate a very long disconnection before
falling behind: each WAL file is sent to both Barman nodes, whose
retention policy is usually specified in terms of days or even weeks,
and standby nodes are configured to retrieve WAL records from Barman
as well as from their upstream nodes.

However, if the Standby node fails permanently, a new standby must be
provisioned by restoring the latest backup from Barman.

In the case of a permanent failure of Node 3, the procedure is
slightly more complex, because Node 3 is the upstream of Node 4, which
will also be disconnected. We recommend the following procedure:

1. Verify the permanent failure of Node 3
2. Reconfigure Node 4 to follow Primary Node 1
3. Remove the failed Node 3 from the `repmgr` catalog using the 
   `standby unregister` action
4. Create a new Standby Node 3' by restoring the latest backup from
   Barman
5. Reconfigure Node 3' so that it follows Node 4

Note that this operation will switch the cascading replication role of
nodes 3 and 4; if this is not desirable, once Node 3' has caught up,
we can restore a configuration more similar to the original one, as
follows:

- Disconnect Node 4 and wait that 3' catches up
- Reconfigure Node 4 so it follows Node 3'
- Reconfigure Node 3' so it follows Node 1

\AuthorComment[GC]{ reviewed up to here }

## Barman Node Failure

On temporary failures of a Barman node, the WAL accumulates on the
sending node, because the Barman streaming connection uses replication
slot.

When the Barman node recovers, it can retrieve all missing WAL files,
and the cluster automatically returns to the original state.

In case of long downtime, however, WAL accumulation will eventually
fill the storage: the PostgreSQL server will crash, unable to recover
until enough disk space is liberated.

Monitoring Barman node failures and reacting soon enough is therefore
very important, even more so considering that they would affect the
Primary Node.

If a Barman node fails and does not recover on time, we must carry out
the following procedure:

1. Drop the corresponding replication slot on the primary node

2. Provision a new Barman node in the same datacenter of the failed
   one, as explained in the [Provisioning the Second Barman Node]
   section.

### Option: Single Barman

If the loss of recovery window is acceptable, it is possible to have
only one Barman node instead of two.

![Option: Single Barman Node\label{ha-1b}](cache/1b.png){ width=85% }

The resulting architecture is described in Figure \ref{ha-1b}. As we
do not have a Barman node in datacenter 2, we lose PITR capability in
case datacenter 1 fails.

## PgBouncer Node Failure

The two PgBouncer nodes represent two independent *entry points* to
the PostgreSQL cluster, one for each datacenter.

Since the cluster has a single Primary Node, the main reason of having
two entry points is for redundancy. In particular, if one entry point
fails, then the application can just use the other one, without having
to perform any cluster-level procedure, as specified in Figure
\ref{ha-epf}.

![PgBouncer Node Failure\label{ha-epf}](cache/epf.png){ width=85% }

Examples of client failover syntax include JDBC, and LibPQ:

* <https://www.postgresql.org/docs/current/static/libpq-connect.html#LIBPQ-MULTIPLE-HOSTS>

For instance, when using LibPQ to specify a sequence of multiple entry
points, the application will react to connection failure by trying all
entry points in sequence until one of them succeeds.

### Alternate Failover Setups

Client Failover is one possible approach to ensuring that the cluster
does not have a single point of failure in its entry point.

While this approach has the merit of robustness without requiring
external components, we mention in this section two alternate
approaches that are commonly used:

- DNS Failover
- Virtual IP Failover

#### DNS Failover

DNS Failover relies on the name resolution layer for mapping a single
hostname to the preferred IP address. This is possible with an
appropriate configuration of the DNS server being used, by specifying
both PgBouncer nodes in the desired order of priority.

For instance, `entrypoint-a` can be resolved to the IP addresses of
PgBouncer A first and PgBouncer B second, while `entrypoint-b` can be
resolved to the same addresses in reverse order. Application servers
can use one name or the other depending on which node is preferred,
e.g. by their physical location.

In case of failure of one of the PgBouncer nodes, this configuration
will automatically redirect connections to the other PgBouncer node,
similarly to what happens with Client Failover.

The reliability of this solution depends on the reliability of the
name resolution layer, which can be achieved in a number of ways:

- configuring resolution statically on each application server
  (e.g. `/etc/hosts`);
- having two independent DNS servers;
- relying on an external DNS service, whose reliability is considered
  sufficient.

#### Virtual IP Failover

A virtual IP address can also be attached to each PgBouncer node, with
a pattern similar to the one described in [DNS Failover], that is:

- VirtualIP-A pointing to PgBouncer A
- VirtualIP-B pointing to PgBouncer B
- Applications using the "closest" Virtual IP

This solution needs an external component which reacts to changes in
the availability of PgBouncer nodes, such as failures, or nodes
becoming available again, and reconfigures the virtual IP as
appropriate.

We note that the robustness of this approach depends on the
reliability of this external component.

## Datacenter Failure

The failure of an entire datacenter can occur by several reasons,
including power outage, planned maintenance, or simply the lost of
connection between both datacenters, which is interpreted by each
datacenter as a failure of the other one. We must also distinguish
between the failure of a datacenter where the Primary Node is running,
in our case "DC A", and the case were only Standby Nodes are running,
i.e. "DC B".

### Failure of Datacenter B

In case of failure of "DC B", the application can still continue
working as the Primary Node, and Standby Node 2 are still working
properly. The first actions that need to be taken are the following:

1. Applications that were connected to PgBouncer in DC B need to
   reconnect through PgBouncer in DC A
2. In case Barman is not redundant, create a new Barman node in DC
   A. This is specially because recovering an entire datacenter might
   take a considerable amount of time. See section
   [Barman Node Failure] for more details

Once those two actions has been taken, create a new Datacenter DC B'
with the same components:

1. New PgBouncer as a clone of PgBouncer node in DC A
2. New Standby Nodes 3' and 4', built by restoring a backup from the
   new Barman node running in DC A
3. Configure Standby Node 4' to follow Standby Node 3' to return to
   the original cascading replication scheme
4. Alternatively, create a new Barmana Node at DC B' for redundancy

### Failure of Datacenter A

If datacenter DC A fails, then the only remaining option is failing
over to datacenter DC B. If the datacenter is fully lost, the
application nodes running in DC A will also be inoperable. In this
procedure we will consider the case where applications connected to DC
A, are running on a different datacenter that is still working.

This is the procedure to failover to Datacenter B in the case of total
loss of Datacenter A, presented in Figure \ref{dc-failure}:

![Datacenter Failure\label{dc-failure}](cache/dcf.png){ width=85% }

1. Verify that DC A is failed, and that DC B is functioning
2. Redirect the application connected to DC A towards PgBouncer Node
   on DC B
3. Promote Node 3 in DC B to become the new Primary Node
4. Reconfigure Barman to follow Node 3, the new Primary

After this procedure, applications are reconfigured to use Datacenter
B, as Figure \ref{dc-failure-2} shows.

![Recovery from Datacenter Failure\label{dc-failure-2}](cache/dcf2.png){ width=85% }

The procedure just described is able to restore operations after
failure of the datacenter where the Primary node is running. However,
the resulting cluster does not have full redundancy capabilities,
because it is vulnerable to a second Datacentre loss. Such issue must
therefore be addressed at the earliest opportunity by provisioning new
nodes in another functioning datacenter. This procedure is very
similar to the one described in [Failure of Datacenter B], because it
builds Standby nodes.

1. New PgBouncer in DC A' as a clone of PgBouncer node in DC B
2. New Standby Nodes 1' and 2', built by restoring a backup from
   Barman running in DC B
3. Configure Standby Node 2' to follow Standby Node 1' to create a
   cascading replication scheme
4. Alternatively, create a new Barman Node at DC A' for redundancy

Note that this procedures switches the roles of DC A and DC B, having
the Primary node recovered at DC B.
